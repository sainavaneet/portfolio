<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MPC | Hugo Academic CV Theme</title>
    <link>http://localhost:1313/tags/mpc/</link>
      <atom:link href="http://localhost:1313/tags/mpc/index.xml" rel="self" type="application/rss+xml" />
    <description>MPC</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 06 Jun 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu_645fa481986063ef.png</url>
      <title>MPC</title>
      <link>http://localhost:1313/tags/mpc/</link>
    </image>
    
    <item>
      <title>üéâ Drone Trajectory Tracking with Python</title>
      <link>http://localhost:1313/post/drone-trajctory-tracking/</link>
      <pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/drone-trajctory-tracking/</guid>
      <description>&lt;h2 id=&#34;1-dynamics-and-kinematics-of-the-drone&#34;&gt;1. Dynamics and Kinematics of the Drone&lt;/h2&gt;
&lt;p&gt;The dynamics and kinematics of a drone are crucial in understanding and controlling its flight behavior. The drone&amp;rsquo;s state can be described by its position $ x, y, z $
, orientation (roll $ \phi $
, pitch $ \theta $
, and yaw $ \psi $
), and their respective velocities and angular velocities.&lt;/p&gt;
&lt;h3 id=&#34;translational-motion&#34;&gt;Translational Motion&lt;/h3&gt;
&lt;p&gt;The translational motion of the drone can be described by Newton&amp;rsquo;s second law of motion:&lt;/p&gt;

$$
m\ddot{\mathbf{r}} = \mathbf{F} - mg\hat{z}
$$


&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ m $
 is the mass of the drone.&lt;/li&gt;
&lt;li&gt;$ \ddot{\mathbf{r}} $
 represents the linear acceleration.&lt;/li&gt;
&lt;li&gt;$ \mathbf{F} $
 is the total thrust force generated by the drone&amp;rsquo;s motors.&lt;/li&gt;
&lt;li&gt;$ g $
 is the acceleration due to gravity.&lt;/li&gt;
&lt;li&gt;$ \hat{z} $
 is the unit vector in the vertical direction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rotational-motion&#34;&gt;Rotational Motion&lt;/h3&gt;
&lt;p&gt;The rotational motion of the drone is described using Euler&amp;rsquo;s equations of motion for a rigid body:&lt;/p&gt;

$$
I \dot{\boldsymbol{\omega}} + \boldsymbol{\omega} \times (I \boldsymbol{\omega}) = \mathbf{\tau}
$$


&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ I $
 represents the moment of inertia matrix.&lt;/li&gt;
&lt;li&gt;$ \boldsymbol{\omega} $
 is the angular velocity vector (p, q, r).&lt;/li&gt;
&lt;li&gt;$ \mathbf{\tau} $
 is the vector of external torques acting on the drone.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dynamic-system-matrix&#34;&gt;Dynamic System Matrix&lt;/h3&gt;
&lt;p&gt;The dynamics of the drone can be represented in a state-space format, where the state vector $ \mathbf{x} $
 might include the position, velocity, orientation, and angular velocity. The state-space model is typically written as:&lt;/p&gt;

$$
\dot{\mathbf{x}} = A\mathbf{x} + B\mathbf{u}
$$



$$
\mathbf{y} = C\mathbf{x} + D\mathbf{u}
$$


&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ \mathbf{x} $
 is the state vector.&lt;/li&gt;
&lt;li&gt;$ \mathbf{u} $
 is the input vector (like motor thrusts).&lt;/li&gt;
&lt;li&gt;$ \mathbf{y} $
 is the output vector (like measured position and orientation).&lt;/li&gt;
&lt;li&gt;$ A $
 is the system matrix, representing the dynamics of the drone.&lt;/li&gt;
&lt;li&gt;$ B $
 is the input matrix, showing how inputs affect the state.&lt;/li&gt;
&lt;li&gt;$ C $
 is the output matrix, linking the state to the outputs.&lt;/li&gt;
&lt;li&gt;$ D $
 is the direct transmission matrix, usually a zero matrix in drone dynamics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The matrices $$ A, B, C, D $$
 are determined based on the physical characteristics of the drone and its operational environment. They encapsulate the dynamics and kinematics, converting control inputs into predictions about the drone&amp;rsquo;s future state.&lt;/p&gt;
&lt;h2 id=&#34;2-control-system&#34;&gt;2. Control System&lt;/h2&gt;
&lt;h3 id=&#34;control-inputs&#34;&gt;Control Inputs&lt;/h3&gt;
&lt;p&gt;The control inputs for the drone include thrust $$ U1 $$
 and torques $$ U2, U3, U4 $$
. These inputs are calculated as follows:&lt;/p&gt;

$$
U1 = c_t(\omega_1^2 + \omega_2^2 + \omega_3^2 + \omega_4^2)
$$



$$
U2 = c_t l (\omega_2^2 - \omega_4^2)
$$



$$
U3 = c_t l (\omega_3^2 - \omega_1^2)
$$



$$
U4 = c_q (-\omega_1^2 + \omega_2^2 - \omega_3^2 + \omega_4^2)
$$


&lt;h2 id=&#34;3-trajectory-generation&#34;&gt;3. Trajectory Generation&lt;/h2&gt;
&lt;p&gt;The desired trajectory is represented as a function of time $ t $
:&lt;/p&gt;

$$
X_{ref}(t), Y_{ref}(t), Z_{ref}(t), \psi_{ref}(t)
$$


&lt;h2 id=&#34;4-linear-parameter-varying-lpv-systems&#34;&gt;4. Linear Parameter Varying (LPV) Systems&lt;/h2&gt;
&lt;p&gt;LPV systems are described by the following equations:&lt;/p&gt;

$$
\dot{x}(t) = A(t)x(t) + B(t)u(t)
$$



$$
y(t) = C(t)x(t) + D(t)u(t)
$$


&lt;h2 id=&#34;5-model-predictive-control-mpc&#34;&gt;5. Model Predictive Control (MPC)&lt;/h2&gt;
&lt;p&gt;Model Predictive Control (MPC) is an advanced method of process control that uses a dynamic model to predict and optimize future states of a control system. In the context of drone trajectory tracking, MPC is used to compute the optimal control inputs that will guide the drone along a desired trajectory. The basic formulation of an MPC problem can be given as follows:&lt;/p&gt;

$$
\min_{u} \sum_{k=0}^{N-1} \left( (x_k - x_{ref})^T Q (x_k - x_{ref}) + (u_k - u_{ref})^T R (u_k - u_{ref}) \right)
$$


&lt;p&gt;Subject to the dynamic constraints of the system:&lt;/p&gt;

$$
x_{k+1} = A x_k + B u_k
$$



$$
x_{min} \leq x_k \leq x_{max}
$$



$$
u_{min} \leq u_k \leq u_{max}
$$


&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ x_k $
 is the state of the system at step $ k $
.&lt;/li&gt;
&lt;li&gt;$ u_k $ is the control input at step $ k $.&lt;/li&gt;
&lt;li&gt;$ x_{ref} $ and $ u_{ref} $ are the reference state and input, respectively.&lt;/li&gt;
&lt;li&gt;$ Q $ and $ R $ are weighting matrices.&lt;/li&gt;
&lt;li&gt;$ N $ is the prediction horizon.&lt;/li&gt;
&lt;li&gt;$ A $ and $ B $ are the system matrices.&lt;/li&gt;
&lt;li&gt;$ x_{min}, x_{max}, u_{min}, u_{max} $ are the bounds on states and inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/S4rpkbglb5c?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>üéâ Drone Trajectory Tracking with Python</title>
      <link>http://localhost:1313/project/drone-trajectory-tracking/</link>
      <pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/drone-trajectory-tracking/</guid>
      <description>&lt;h2 id=&#34;1-dynamics-and-kinematics-of-the-drone&#34;&gt;1. Dynamics and Kinematics of the Drone&lt;/h2&gt;
&lt;p&gt;The dynamics and kinematics of a drone are crucial in understanding and controlling its flight behavior. The drone&amp;rsquo;s state can be described by its position $ x, y, z $
, orientation (roll $ \phi $
, pitch $ \theta $
, and yaw $ \psi $
), and their respective velocities and angular velocities.&lt;/p&gt;
&lt;h3 id=&#34;translational-motion&#34;&gt;Translational Motion&lt;/h3&gt;
&lt;p&gt;The translational motion of the drone can be described by Newton&amp;rsquo;s second law of motion:&lt;/p&gt;

$$
m\ddot{\mathbf{r}} = \mathbf{F} - mg\hat{z}
$$


&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ m $
 is the mass of the drone.&lt;/li&gt;
&lt;li&gt;$ \ddot{\mathbf{r}} $
 represents the linear acceleration.&lt;/li&gt;
&lt;li&gt;$ \mathbf{F} $
 is the total thrust force generated by the drone&amp;rsquo;s motors.&lt;/li&gt;
&lt;li&gt;$ g $
 is the acceleration due to gravity.&lt;/li&gt;
&lt;li&gt;$ \hat{z} $
 is the unit vector in the vertical direction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rotational-motion&#34;&gt;Rotational Motion&lt;/h3&gt;
&lt;p&gt;The rotational motion of the drone is described using Euler&amp;rsquo;s equations of motion for a rigid body:&lt;/p&gt;

$$
I \dot{\boldsymbol{\omega}} + \boldsymbol{\omega} \times (I \boldsymbol{\omega}) = \mathbf{\tau}
$$


&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ I $
 represents the moment of inertia matrix.&lt;/li&gt;
&lt;li&gt;$ \boldsymbol{\omega} $
 is the angular velocity vector (p, q, r).&lt;/li&gt;
&lt;li&gt;$ \mathbf{\tau} $
 is the vector of external torques acting on the drone.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dynamic-system-matrix&#34;&gt;Dynamic System Matrix&lt;/h3&gt;
&lt;p&gt;The dynamics of the drone can be represented in a state-space format, where the state vector $ \mathbf{x} $
 might include the position, velocity, orientation, and angular velocity. The state-space model is typically written as:&lt;/p&gt;

$$
\dot{\mathbf{x}} = A\mathbf{x} + B\mathbf{u}
$$



$$
\mathbf{y} = C\mathbf{x} + D\mathbf{u}
$$


&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ \mathbf{x} $
 is the state vector.&lt;/li&gt;
&lt;li&gt;$ \mathbf{u} $
 is the input vector (like motor thrusts).&lt;/li&gt;
&lt;li&gt;$ \mathbf{y} $
 is the output vector (like measured position and orientation).&lt;/li&gt;
&lt;li&gt;$ A $
 is the system matrix, representing the dynamics of the drone.&lt;/li&gt;
&lt;li&gt;$ B $
 is the input matrix, showing how inputs affect the state.&lt;/li&gt;
&lt;li&gt;$ C $
 is the output matrix, linking the state to the outputs.&lt;/li&gt;
&lt;li&gt;$ D $
 is the direct transmission matrix, usually a zero matrix in drone dynamics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The matrices $$ A, B, C, D $$
 are determined based on the physical characteristics of the drone and its operational environment. They encapsulate the dynamics and kinematics, converting control inputs into predictions about the drone&amp;rsquo;s future state.&lt;/p&gt;
&lt;h2 id=&#34;2-control-system&#34;&gt;2. Control System&lt;/h2&gt;
&lt;h3 id=&#34;control-inputs&#34;&gt;Control Inputs&lt;/h3&gt;
&lt;p&gt;The control inputs for the drone include thrust $$ U1 $$
 and torques $$ U2, U3, U4 $$
. These inputs are calculated as follows:&lt;/p&gt;

$$
U1 = c_t(\omega_1^2 + \omega_2^2 + \omega_3^2 + \omega_4^2)
$$



$$
U2 = c_t l (\omega_2^2 - \omega_4^2)
$$



$$
U3 = c_t l (\omega_3^2 - \omega_1^2)
$$



$$
U4 = c_q (-\omega_1^2 + \omega_2^2 - \omega_3^2 + \omega_4^2)
$$


&lt;h2 id=&#34;3-trajectory-generation&#34;&gt;3. Trajectory Generation&lt;/h2&gt;
&lt;p&gt;The desired trajectory is represented as a function of time $ t $
:&lt;/p&gt;

$$
X_{ref}(t), Y_{ref}(t), Z_{ref}(t), \psi_{ref}(t)
$$


&lt;h2 id=&#34;4-linear-parameter-varying-lpv-systems&#34;&gt;4. Linear Parameter Varying (LPV) Systems&lt;/h2&gt;
&lt;p&gt;LPV systems are described by the following equations:&lt;/p&gt;

$$
\dot{x}(t) = A(t)x(t) + B(t)u(t)
$$



$$
y(t) = C(t)x(t) + D(t)u(t)
$$


&lt;h2 id=&#34;5-model-predictive-control-mpc&#34;&gt;5. Model Predictive Control (MPC)&lt;/h2&gt;
&lt;p&gt;Model Predictive Control (MPC) is an advanced method of process control that uses a dynamic model to predict and optimize future states of a control system. In the context of drone trajectory tracking, MPC is used to compute the optimal control inputs that will guide the drone along a desired trajectory. The basic formulation of an MPC problem can be given as follows:&lt;/p&gt;

$$
\min_{u} \sum_{k=0}^{N-1} \left( (x_k - x_{ref})^T Q (x_k - x_{ref}) + (u_k - u_{ref})^T R (u_k - u_{ref}) \right)
$$


&lt;p&gt;Subject to the dynamic constraints of the system:&lt;/p&gt;

$$
x_{k+1} = A x_k + B u_k
$$



$$
x_{min} \leq x_k \leq x_{max}
$$



$$
u_{min} \leq u_k \leq u_{max}
$$


&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ x_k $
 is the state of the system at step $ k $
.&lt;/li&gt;
&lt;li&gt;$ u_k $ is the control input at step $ k $.&lt;/li&gt;
&lt;li&gt;$ x_{ref} $ and $ u_{ref} $ are the reference state and input, respectively.&lt;/li&gt;
&lt;li&gt;$ Q $ and $ R $ are weighting matrices.&lt;/li&gt;
&lt;li&gt;$ N $ is the prediction horizon.&lt;/li&gt;
&lt;li&gt;$ A $ and $ B $ are the system matrices.&lt;/li&gt;
&lt;li&gt;$ x_{min}, x_{max}, u_{min}, u_{max} $ are the bounds on states and inputs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/S4rpkbglb5c?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>üè† Smart-Home-Using-IOT</title>
      <link>http://localhost:1313/post/smart-home/</link>
      <pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/smart-home/</guid>
      <description>&lt;h2 id=&#34;project-overview&#34;&gt;Project Overview&lt;/h2&gt;
&lt;p&gt;Project Name: Home Automation (Smart Home Using Internet of Things)&lt;/p&gt;
&lt;p&gt;Participatory Department: Electronics Engineering, Electrical Engineering&lt;/p&gt;
&lt;p&gt;Project Representative: Sai Navaneet&lt;/p&gt;
&lt;p&gt;Participants:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corporation: 1&lt;/li&gt;
&lt;li&gt;Professor: 1&lt;/li&gt;
&lt;li&gt;Undergraduate students: 2 (Manisha Lingala, Navaneet)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Period: March 1, 2022, to July 31, 2022 (5 months)&lt;/p&gt;
&lt;p&gt;Project Type: Middle&lt;/p&gt;
&lt;h2 id=&#34;project-background&#34;&gt;Project Background&lt;/h2&gt;
&lt;p&gt;The concept of home automation has been around for a long time, with the vision of fully automated homes and robotic assistance in household chores. However, the technology required to realize these ideas was not readily available until recent times. The project aims to leverage IoT technology to build a working intelligent home application, starting from scratch. The idea of smart homes has been experimented with since the early 19th century, aiming to make lives more comfortable.&lt;/p&gt;
&lt;h2 id=&#34;objectives-and-content&#34;&gt;Objectives and Content&lt;/h2&gt;
&lt;p&gt;The main objectives of the project are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Increase comfort and quality of life in the house&lt;/li&gt;
&lt;li&gt;Enhance security and energy efficiency through remote-controllable equipment&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To achieve these objectives, the following components are required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relays&lt;/li&gt;
&lt;li&gt;Wi-Fi module (ESP32)&lt;/li&gt;
&lt;li&gt;Other basic electronic components&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-results&#34;&gt;Project Results&lt;/h2&gt;
&lt;p&gt;The project successfully achieved the following outcomes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Built a model of a house with basic appliances such as lights and fans&lt;/li&gt;
&lt;li&gt;Enabled remote control of these appliances through the cloud from anywhere in the world&lt;/li&gt;
&lt;li&gt;Implemented four types of control methods:
&lt;ul&gt;
&lt;li&gt;Control through the cloud&lt;/li&gt;
&lt;li&gt;Control through a mobile app&lt;/li&gt;
&lt;li&gt;Voice control&lt;/li&gt;
&lt;li&gt;Time-based control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;circuit-diagram&#34;&gt;Circuit Diagram&lt;/h2&gt;
&lt;p&gt;Below is the circuit diagram of the smart home system:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Circuit Diagram&#34; srcset=&#34;
               /post/smart-home/images/circuit_hu_fe8e23845ccf5a4c.webp 400w,
               /post/smart-home/images/circuit_hu_63c4c723cd46e799.webp 760w,
               /post/smart-home/images/circuit_hu_b62cac56fc4acf82.webp 1200w&#34;
               src=&#34;http://localhost:1313/post/smart-home/images/circuit_hu_fe8e23845ccf5a4c.webp&#34;
               width=&#34;736&#34;
               height=&#34;402&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The circuit diagram showcases the connectivity of various components involved in the project. It includes the relays, Wi-Fi module (ESP32), and other basic electronic components. This diagram provides a visual representation of how the different elements of the system are interconnected to enable the desired functionality.&lt;/p&gt;
&lt;h2 id=&#34;control-methods&#34;&gt;Control Methods&lt;/h2&gt;
&lt;p&gt;The project implemented three different control methods for operating the smart home system:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Control through the Cloud:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Control through the Cloud&#34; srcset=&#34;
               /post/smart-home/images/appESP32_hu_cd0c3eabc49e6947.webp 400w,
               /post/smart-home/images/appESP32_hu_beaa95f9a3223ab3.webp 760w,
               /post/smart-home/images/appESP32_hu_a62ccc4362a512d6.webp 1200w&#34;
               src=&#34;http://localhost:1313/post/smart-home/images/appESP32_hu_cd0c3eabc49e6947.webp&#34;
               width=&#34;330&#34;
               height=&#34;722&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This method allows users to control the smart home appliances using a web-based interface or application. Users can access the control interface through any internet-connected device, such as a computer or smartphone. It provides convenience and flexibility for managing the devices remotely, even in the absence of a mobile phone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Control through Voice:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Control through Voice&#34; srcset=&#34;
               /post/smart-home/images/blynk_hu_6607d9ec66dd0d00.webp 400w,
               /post/smart-home/images/blynk_hu_85c8c174c0345a81.webp 760w,
               /post/smart-home/images/blynk_hu_c496687e8081e683.webp 1200w&#34;
               src=&#34;http://localhost:1313/post/smart-home/images/blynk_hu_6607d9ec66dd0d00.webp&#34;
               width=&#34;422&#34;
               height=&#34;156&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Voice control was implemented using Google Assistant and the IFTTT (If This Then That) platform. Users can give voice commands to control the devices. By assigning specific voice commands for turning on or off the appliances, users can conveniently operate the smart home system using their voice.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Control through Timer:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Control through Timer&#34; srcset=&#34;
               /post/smart-home/images/app2_hu_9772d045fb77c729.webp 400w,
               /post/smart-home/images/app2_hu_fdd9a0832828614b.webp 760w,
               /post/smart-home/images/app2_hu_b0d3b225f87fb83a.webp 1200w&#34;
               src=&#34;http://localhost:1313/post/smart-home/images/app2_hu_9772d045fb77c729.webp&#34;
               width=&#34;258&#34;
               height=&#34;324&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The timer control method enables users to schedule the activation of devices at specific times. Users can set a particular time for the devices to turn on automatically. This feature is useful for automating routines and ensuring that appliances are activated at desired times without manual intervention.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;final-model&#34;&gt;Final Model&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Final Model&#34; srcset=&#34;
               /post/smart-home/images/model_hu_679de8265b7eda3e.webp 400w,
               /post/smart-home/images/model_hu_a12d1c64ff7e8631.webp 760w,
               /post/smart-home/images/model_hu_4a3b364805ccb0df.webp 1200w&#34;
               src=&#34;http://localhost:1313/post/smart-home/images/model_hu_679de8265b7eda3e.webp&#34;
               width=&#34;451&#34;
               height=&#34;449&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üè† Smart-Home-Using-IOT</title>
      <link>http://localhost:1313/project/smart-home-using-iot/</link>
      <pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/smart-home-using-iot/</guid>
      <description>&lt;h2 id=&#34;project-overview&#34;&gt;Project Overview&lt;/h2&gt;
&lt;p&gt;Project Name: Home Automation (Smart Home Using Internet of Things)&lt;/p&gt;
&lt;p&gt;Participatory Department: Electronics Engineering, Electrical Engineering&lt;/p&gt;
&lt;p&gt;Project Representative: Sai Navaneet&lt;/p&gt;
&lt;p&gt;Participants:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Corporation: 1&lt;/li&gt;
&lt;li&gt;Professor: 1&lt;/li&gt;
&lt;li&gt;Undergraduate students: 2 (Manisha Lingala, Navaneet)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Period: March 1, 2022, to July 31, 2022 (5 months)&lt;/p&gt;
&lt;p&gt;Project Type: Middle&lt;/p&gt;
&lt;h2 id=&#34;project-background&#34;&gt;Project Background&lt;/h2&gt;
&lt;p&gt;The concept of home automation has been around for a long time, with the vision of fully automated homes and robotic assistance in household chores. However, the technology required to realize these ideas was not readily available until recent times. The project aims to leverage IoT technology to build a working intelligent home application, starting from scratch. The idea of smart homes has been experimented with since the early 19th century, aiming to make lives more comfortable.&lt;/p&gt;
&lt;h2 id=&#34;objectives-and-content&#34;&gt;Objectives and Content&lt;/h2&gt;
&lt;p&gt;The main objectives of the project are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Increase comfort and quality of life in the house&lt;/li&gt;
&lt;li&gt;Enhance security and energy efficiency through remote-controllable equipment&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To achieve these objectives, the following components are required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relays&lt;/li&gt;
&lt;li&gt;Wi-Fi module (ESP32)&lt;/li&gt;
&lt;li&gt;Other basic electronic components&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-results&#34;&gt;Project Results&lt;/h2&gt;
&lt;p&gt;The project successfully achieved the following outcomes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Built a model of a house with basic appliances such as lights and fans&lt;/li&gt;
&lt;li&gt;Enabled remote control of these appliances through the cloud from anywhere in the world&lt;/li&gt;
&lt;li&gt;Implemented four types of control methods:
&lt;ul&gt;
&lt;li&gt;Control through the cloud&lt;/li&gt;
&lt;li&gt;Control through a mobile app&lt;/li&gt;
&lt;li&gt;Voice control&lt;/li&gt;
&lt;li&gt;Time-based control&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;circuit-diagram&#34;&gt;Circuit Diagram&lt;/h2&gt;
&lt;p&gt;Below is the circuit diagram of the smart home system:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Circuit Diagram&#34; srcset=&#34;
               /project/smart-home-using-iot/images/circuit_hu_fe8e23845ccf5a4c.webp 400w,
               /project/smart-home-using-iot/images/circuit_hu_63c4c723cd46e799.webp 760w,
               /project/smart-home-using-iot/images/circuit_hu_b62cac56fc4acf82.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/smart-home-using-iot/images/circuit_hu_fe8e23845ccf5a4c.webp&#34;
               width=&#34;736&#34;
               height=&#34;402&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The circuit diagram showcases the connectivity of various components involved in the project. It includes the relays, Wi-Fi module (ESP32), and other basic electronic components. This diagram provides a visual representation of how the different elements of the system are interconnected to enable the desired functionality.&lt;/p&gt;
&lt;h2 id=&#34;control-methods&#34;&gt;Control Methods&lt;/h2&gt;
&lt;p&gt;The project implemented three different control methods for operating the smart home system:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Control through the Cloud:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Control through the Cloud&#34; srcset=&#34;
               /project/smart-home-using-iot/images/appESP32_hu_cd0c3eabc49e6947.webp 400w,
               /project/smart-home-using-iot/images/appESP32_hu_beaa95f9a3223ab3.webp 760w,
               /project/smart-home-using-iot/images/appESP32_hu_a62ccc4362a512d6.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/smart-home-using-iot/images/appESP32_hu_cd0c3eabc49e6947.webp&#34;
               width=&#34;330&#34;
               height=&#34;722&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This method allows users to control the smart home appliances using a web-based interface or application. Users can access the control interface through any internet-connected device, such as a computer or smartphone. It provides convenience and flexibility for managing the devices remotely, even in the absence of a mobile phone.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Control through Voice:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Control through Voice&#34; srcset=&#34;
               /project/smart-home-using-iot/images/blynk_hu_6607d9ec66dd0d00.webp 400w,
               /project/smart-home-using-iot/images/blynk_hu_85c8c174c0345a81.webp 760w,
               /project/smart-home-using-iot/images/blynk_hu_c496687e8081e683.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/smart-home-using-iot/images/blynk_hu_6607d9ec66dd0d00.webp&#34;
               width=&#34;422&#34;
               height=&#34;156&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Voice control was implemented using Google Assistant and the IFTTT (If This Then That) platform. Users can give voice commands to control the devices. By assigning specific voice commands for turning on or off the appliances, users can conveniently operate the smart home system using their voice.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Control through Timer:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Control through Timer&#34; srcset=&#34;
               /project/smart-home-using-iot/images/app2_hu_9772d045fb77c729.webp 400w,
               /project/smart-home-using-iot/images/app2_hu_fdd9a0832828614b.webp 760w,
               /project/smart-home-using-iot/images/app2_hu_b0d3b225f87fb83a.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/smart-home-using-iot/images/app2_hu_9772d045fb77c729.webp&#34;
               width=&#34;258&#34;
               height=&#34;324&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The timer control method enables users to schedule the activation of devices at specific times. Users can set a particular time for the devices to turn on automatically. This feature is useful for automating routines and ensuring that appliances are activated at desired times without manual intervention.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;final-model&#34;&gt;Final Model&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Final Model&#34; srcset=&#34;
               /project/smart-home-using-iot/images/model_hu_679de8265b7eda3e.webp 400w,
               /project/smart-home-using-iot/images/model_hu_a12d1c64ff7e8631.webp 760w,
               /project/smart-home-using-iot/images/model_hu_4a3b364805ccb0df.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/smart-home-using-iot/images/model_hu_679de8265b7eda3e.webp&#34;
               width=&#34;451&#34;
               height=&#34;449&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hybrid Model Predictive and Iterative Learning Control for Enhanced Leader-Follower Robotic Tracking</title>
      <link>http://localhost:1313/project/lmpc-ilc/</link>
      <pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/lmpc-ilc/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This paper introduces a novel hybrid control strategy combining Model Predictive Control (MPC) and Iterative Learning Control (ILC) to improve leader-follower tracking accuracy and robustness in mobile robots. MPC optimizes control inputs over a predictive horizon by minimizing a cost function, while ILC refines these inputs by learning from past errors.&lt;/p&gt;
&lt;h2 id=&#34;mathematical-formulation&#34;&gt;Mathematical Formulation&lt;/h2&gt;
&lt;h3 id=&#34;system-dynamics&#34;&gt;System Dynamics&lt;/h3&gt;
&lt;p&gt;The state of the follower robot is:
$$
x_f = [x_f, y_f, \theta_f]^T
$$
&lt;/p&gt;
&lt;p&gt;Control inputs:
$$
u = [v, \omega]^T$$
&lt;/p&gt;
&lt;p&gt;Discrete-time kinematic model:
$$
x_f(k + 1) = x_f(k) + \begin{bmatrix}
v(k) \cos(\theta_f(k)) T \\
v(k) \sin(\theta_f(k)) T \\
\omega(k) T
\end{bmatrix}
$$
&lt;/p&gt;
&lt;p&gt;where $T$
 is the sampling time.&lt;/p&gt;
&lt;h3 id=&#34;mpc-cost-function&#34;&gt;MPC Cost Function&lt;/h3&gt;
&lt;p&gt;The MPC minimizes the following cost function over a finite horizon $N$
:&lt;/p&gt;
$$
J = \sum_{k=0}^{N-1}\left[\omega_p\|P_f(k)-P_d(k)\|^2 + \omega_\theta(\theta_f(k)-\theta_d(k))^2 + \omega_u\|u(k)\|^2\right]
$$

&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P_f(k) = [x_f(k), y_f(k)]^T$
: position of follower.&lt;/li&gt;
&lt;li&gt;$P_d(k), \theta_d(k)$
: desired position and orientation.&lt;/li&gt;
&lt;li&gt;$\omega_p, \omega_\theta, \omega_u$
: weights for position, orientation errors, and control effort.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;desired-position-calculation&#34;&gt;Desired Position Calculation&lt;/h3&gt;
&lt;p&gt;The desired position $P_d$
 is calculated from the leader&amp;rsquo;s state $x_l = [x_l, y_l, \theta_l]^T$
 and specified distance $d$
:&lt;/p&gt;
$$
P_d = \begin{bmatrix}
x_l - d\cos(\theta_l) \\
y_l - d\sin(\theta_l)
\end{bmatrix}
$$

&lt;h3 id=&#34;iterative-learning-control-ilc-update&#34;&gt;Iterative Learning Control (ILC) Update&lt;/h3&gt;
&lt;p&gt;Error vector:
$$
e(k) = \begin{bmatrix}
x_f(k)-x_d(k) \\
y_f(k)-y_d(k) \\
\theta_f(k)-\theta_d(k)
\end{bmatrix}
$$
&lt;/p&gt;
&lt;p&gt;ILC control update:
$$
u_{ILC}(k)=\nu_{MPC}(k)+L[e(k)-e(k-1)]$$
&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\nu_{MPC}(k)$
: optimal MPC control input.&lt;/li&gt;
&lt;li&gt;$L$
: learning matrix.&lt;/li&gt;
&lt;li&gt;$e(k-1)$
: error from previous iteration.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hybrid-integration&#34;&gt;Hybrid Integration&lt;/h3&gt;
&lt;p&gt;The hybrid method combines MPC and ILC:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Solve MPC optimization: $\nu_{MPC}=\arg\min J$
&lt;/li&gt;
&lt;li&gt;ILC update: $\nu=\nu_{ILC}$
&lt;/li&gt;
&lt;li&gt;Apply $\nu$
 to follower robot.&lt;/li&gt;
&lt;li&gt;Update follower and leader states.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Algorithm 1: Hybrid MPC-ILC
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Require: Initial states x_f, x_l; past error e_past=0, past input u_past=0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Require: T, N, \omega_p, \omega_\theta, \omega_u, d, L
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;While true:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  1. Obtain current leader state x_l(k)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  2. Compute desired P_d(k), \theta_d(k)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  3. Solve MPC to get u_MPC
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  4. Calculate tracking error e(k)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  5. Update control: u_ILC = u_past + L(e(k)-e_past)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  6. Apply u_ILC to follower
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  7. Update u_past = u_MPC, e_past = e(k)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  8. Update robot states
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;End While
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;experimental-results&#34;&gt;Experimental Results&lt;/h2&gt;
&lt;p&gt;Experiments with TurtleBot 3 robots validated the hybrid MPC-ILC method, demonstrating significant improvements in position tracking accuracy, orientation accuracy, and balanced control efforts compared to standalone MPC and ILC.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leader velocity: $v=0.3\,m/s, \omega=0.3\,rad/s$
&lt;/li&gt;
&lt;li&gt;MPC parameters: $N=20, T=0.2\,s, \omega_p=7, \omega_\theta=1, \omega_u=1$
&lt;/li&gt;
&lt;li&gt;ILC learning matrix: $L=diag([0.1, 0.1])$
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Comparision of Cost&#34; srcset=&#34;
               /project/lmpc-ilc/cost_comparision_hu_cc29fef1e91ed160.webp 400w,
               /project/lmpc-ilc/cost_comparision_hu_ad61cc05323d8926.webp 760w,
               /project/lmpc-ilc/cost_comparision_hu_6f496aea44637029.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/lmpc-ilc/cost_comparision_hu_cc29fef1e91ed160.webp&#34;
               width=&#34;760&#34;
               height=&#34;383&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Results&#34; srcset=&#34;
               /project/lmpc-ilc/graph_hu_b28cc522351f3093.webp 400w,
               /project/lmpc-ilc/graph_hu_6d842a55e7f75780.webp 760w,
               /project/lmpc-ilc/graph_hu_3ff6d4a63e846f73.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/lmpc-ilc/graph_hu_b28cc522351f3093.webp&#34;
               width=&#34;760&#34;
               height=&#34;585&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The hybrid MPC-ILC approach offers improved accuracy, robustness, and efficiency for leader-follower robotic tracking. Future work includes integrating with advanced control methods and testing in more complex scenarios.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/sainavaneet/MPC-ILC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/CdYn9fnHEcE?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;hr&gt;
&lt;h2 id=&#34;comparison-between-mpc-and-lmpc-and-obstacle-avoidance&#34;&gt;comparison between MPC and LMPC and Obstacle Avoidance&lt;/h2&gt;
&lt;p&gt;We included a safe set process to keep the follower robot at a fixed distance from the leader robot, and then we added obstacle avoidance to the follower robot using LMPC.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/v308ep-aqd0?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h5 id=&#34;testing-with-mpc&#34;&gt;Testing With MPC&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Testing with MPC&#34; srcset=&#34;
               /project/lmpc-ilc/mpc_hu_7e99abd61dd302f6.webp 400w,
               /project/lmpc-ilc/mpc_hu_ef3e1bf245f5fba0.webp 760w,
               /project/lmpc-ilc/mpc_hu_a524ac5752021547.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/lmpc-ilc/mpc_hu_7e99abd61dd302f6.webp&#34;
               width=&#34;760&#34;
               height=&#34;448&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;testing-with-lmpc&#34;&gt;Testing with LMPC&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/lmpc-ilc/lmpc_hu_41484c930b625b3.webp 400w,
               /project/lmpc-ilc/lmpc_hu_30b7ec01f54893af.webp 760w,
               /project/lmpc-ilc/lmpc_hu_db0b2a78ba999310.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/lmpc-ilc/lmpc_hu_41484c930b625b3.webp&#34;
               width=&#34;760&#34;
               height=&#34;448&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/lmpc-ilc/lmpc1_hu_42294906efbf00ea.webp 400w,
               /project/lmpc-ilc/lmpc1_hu_bc31b697cebe38b8.webp 760w,
               /project/lmpc-ilc/lmpc1_hu_24067942e2203cda.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/lmpc-ilc/lmpc1_hu_42294906efbf00ea.webp&#34;
               width=&#34;760&#34;
               height=&#34;448&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/lmpc-ilc/lmpc2_hu_a6429b4ba8d2f897.webp 400w,
               /project/lmpc-ilc/lmpc2_hu_cafa789bee9fc049.webp 760w,
               /project/lmpc-ilc/lmpc2_hu_1908a943b1dc04c0.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/lmpc-ilc/lmpc2_hu_a6429b4ba8d2f897.webp&#34;
               width=&#34;760&#34;
               height=&#34;519&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tested-for-s-trajectory&#34;&gt;Tested for &amp;ldquo;S&amp;rdquo; Trajectory&lt;/h2&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/6Dyg_3ztMKw?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;comparision combined&#34; srcset=&#34;
               /project/lmpc-ilc/comparision_combined_image_hu_1e5e63cdd8faec82.webp 400w,
               /project/lmpc-ilc/comparision_combined_image_hu_47b76dc380193d32.webp 760w,
               /project/lmpc-ilc/comparision_combined_image_hu_7bc68d2ec2d046f1.webp 1200w&#34;
               src=&#34;http://localhost:1313/project/lmpc-ilc/comparision_combined_image_hu_1e5e63cdd8faec82.webp&#34;
               width=&#34;760&#34;
               height=&#34;73&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
