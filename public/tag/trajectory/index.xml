<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Trajectory | FLORENCIA GRATTAROLA</title><link>http://localhost:1313/tag/trajectory/</link><atom:link href="http://localhost:1313/tag/trajectory/index.xml" rel="self" type="application/rss+xml"/><description>Trajectory</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 06 Jun 2023 00:00:00 +0000</lastBuildDate><image><url>http://localhost:1313/media/icon_hu_2ac6985030d8f1fc.png</url><title>Trajectory</title><link>http://localhost:1313/tag/trajectory/</link></image><item><title>🎉 Drone Trajectory Tracking with Python</title><link>http://localhost:1313/post/drone-trajctory-tracking/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/drone-trajctory-tracking/</guid><description>&lt;h2 id="1-dynamics-and-kinematics-of-the-drone">1. Dynamics and Kinematics of the Drone&lt;/h2>
&lt;p>The dynamics and kinematics of a drone are crucial in understanding and controlling its flight behavior. The drone&amp;rsquo;s state can be described by its position $ x, y, z $
, orientation (roll $ \phi $
, pitch $ \theta $
, and yaw $ \psi $
), and their respective velocities and angular velocities.&lt;/p>
&lt;h3 id="translational-motion">Translational Motion&lt;/h3>
&lt;p>The translational motion of the drone can be described by Newton&amp;rsquo;s second law of motion:&lt;/p>
$$
m\ddot{\mathbf{r}} = \mathbf{F} - mg\hat{z}
$$
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$ m $
is the mass of the drone.&lt;/li>
&lt;li>$ \ddot{\mathbf{r}} $
represents the linear acceleration.&lt;/li>
&lt;li>$ \mathbf{F} $
is the total thrust force generated by the drone&amp;rsquo;s motors.&lt;/li>
&lt;li>$ g $
is the acceleration due to gravity.&lt;/li>
&lt;li>$ \hat{z} $
is the unit vector in the vertical direction.&lt;/li>
&lt;/ul>
&lt;h3 id="rotational-motion">Rotational Motion&lt;/h3>
&lt;p>The rotational motion of the drone is described using Euler&amp;rsquo;s equations of motion for a rigid body:&lt;/p>
$$
I \dot{\boldsymbol{\omega}} + \boldsymbol{\omega} \times (I \boldsymbol{\omega}) = \mathbf{\tau}
$$
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$ I $
represents the moment of inertia matrix.&lt;/li>
&lt;li>$ \boldsymbol{\omega} $
is the angular velocity vector (p, q, r).&lt;/li>
&lt;li>$ \mathbf{\tau} $
is the vector of external torques acting on the drone.&lt;/li>
&lt;/ul>
&lt;h3 id="dynamic-system-matrix">Dynamic System Matrix&lt;/h3>
&lt;p>The dynamics of the drone can be represented in a state-space format, where the state vector $ \mathbf{x} $
might include the position, velocity, orientation, and angular velocity. The state-space model is typically written as:&lt;/p>
$$
\dot{\mathbf{x}} = A\mathbf{x} + B\mathbf{u}
$$
$$
\mathbf{y} = C\mathbf{x} + D\mathbf{u}
$$
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$ \mathbf{x} $
is the state vector.&lt;/li>
&lt;li>$ \mathbf{u} $
is the input vector (like motor thrusts).&lt;/li>
&lt;li>$ \mathbf{y} $
is the output vector (like measured position and orientation).&lt;/li>
&lt;li>$ A $
is the system matrix, representing the dynamics of the drone.&lt;/li>
&lt;li>$ B $
is the input matrix, showing how inputs affect the state.&lt;/li>
&lt;li>$ C $
is the output matrix, linking the state to the outputs.&lt;/li>
&lt;li>$ D $
is the direct transmission matrix, usually a zero matrix in drone dynamics.&lt;/li>
&lt;/ul>
&lt;p>The matrices $$ A, B, C, D $$
are determined based on the physical characteristics of the drone and its operational environment. They encapsulate the dynamics and kinematics, converting control inputs into predictions about the drone&amp;rsquo;s future state.&lt;/p>
&lt;h2 id="2-control-system">2. Control System&lt;/h2>
&lt;h3 id="control-inputs">Control Inputs&lt;/h3>
&lt;p>The control inputs for the drone include thrust $$ U1 $$
and torques $$ U2, U3, U4 $$
. These inputs are calculated as follows:&lt;/p>
$$
U1 = c_t(\omega_1^2 + \omega_2^2 + \omega_3^2 + \omega_4^2)
$$
$$
U2 = c_t l (\omega_2^2 - \omega_4^2)
$$
$$
U3 = c_t l (\omega_3^2 - \omega_1^2)
$$
$$
U4 = c_q (-\omega_1^2 + \omega_2^2 - \omega_3^2 + \omega_4^2)
$$
&lt;h2 id="3-trajectory-generation">3. Trajectory Generation&lt;/h2>
&lt;p>The desired trajectory is represented as a function of time $ t $
:&lt;/p>
$$
X_{ref}(t), Y_{ref}(t), Z_{ref}(t), \psi_{ref}(t)
$$
&lt;h2 id="4-linear-parameter-varying-lpv-systems">4. Linear Parameter Varying (LPV) Systems&lt;/h2>
&lt;p>LPV systems are described by the following equations:&lt;/p>
$$
\dot{x}(t) = A(t)x(t) + B(t)u(t)
$$
$$
y(t) = C(t)x(t) + D(t)u(t)
$$
&lt;h2 id="5-model-predictive-control-mpc">5. Model Predictive Control (MPC)&lt;/h2>
&lt;p>Model Predictive Control (MPC) is an advanced method of process control that uses a dynamic model to predict and optimize future states of a control system. In the context of drone trajectory tracking, MPC is used to compute the optimal control inputs that will guide the drone along a desired trajectory. The basic formulation of an MPC problem can be given as follows:&lt;/p>
$$
\min_{u} \sum_{k=0}^{N-1} \left( (x_k - x_{ref})^T Q (x_k - x_{ref}) + (u_k - u_{ref})^T R (u_k - u_{ref}) \right)
$$
&lt;p>Subject to the dynamic constraints of the system:&lt;/p>
$$
x_{k+1} = A x_k + B u_k
$$
$$
x_{min} \leq x_k \leq x_{max}
$$
$$
u_{min} \leq u_k \leq u_{max}
$$
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$ x_k $
is the state of the system at step $ k $
.&lt;/li>
&lt;li>$ u_k $ is the control input at step $ k $.&lt;/li>
&lt;li>$ x_{ref} $ and $ u_{ref} $ are the reference state and input, respectively.&lt;/li>
&lt;li>$ Q $ and $ R $ are weighting matrices.&lt;/li>
&lt;li>$ N $ is the prediction horizon.&lt;/li>
&lt;li>$ A $ and $ B $ are the system matrices.&lt;/li>
&lt;li>$ x_{min}, x_{max}, u_{min}, u_{max} $ are the bounds on states and inputs.&lt;/li>
&lt;/ul>
&lt;h2 id="results">Results&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/S4rpkbglb5c?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div></description></item><item><title>🎉 Drone Trajectory Tracking with Python</title><link>http://localhost:1313/project/drone-trajectory-tracking/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/project/drone-trajectory-tracking/</guid><description>&lt;h2 id="1-dynamics-and-kinematics-of-the-drone">1. Dynamics and Kinematics of the Drone&lt;/h2>
&lt;p>The dynamics and kinematics of a drone are crucial in understanding and controlling its flight behavior. The drone&amp;rsquo;s state can be described by its position $ x, y, z $
, orientation (roll $ \phi $
, pitch $ \theta $
, and yaw $ \psi $
), and their respective velocities and angular velocities.&lt;/p>
&lt;h3 id="translational-motion">Translational Motion&lt;/h3>
&lt;p>The translational motion of the drone can be described by Newton&amp;rsquo;s second law of motion:&lt;/p>
$$
m\ddot{\mathbf{r}} = \mathbf{F} - mg\hat{z}
$$
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$ m $
is the mass of the drone.&lt;/li>
&lt;li>$ \ddot{\mathbf{r}} $
represents the linear acceleration.&lt;/li>
&lt;li>$ \mathbf{F} $
is the total thrust force generated by the drone&amp;rsquo;s motors.&lt;/li>
&lt;li>$ g $
is the acceleration due to gravity.&lt;/li>
&lt;li>$ \hat{z} $
is the unit vector in the vertical direction.&lt;/li>
&lt;/ul>
&lt;h3 id="rotational-motion">Rotational Motion&lt;/h3>
&lt;p>The rotational motion of the drone is described using Euler&amp;rsquo;s equations of motion for a rigid body:&lt;/p>
$$
I \dot{\boldsymbol{\omega}} + \boldsymbol{\omega} \times (I \boldsymbol{\omega}) = \mathbf{\tau}
$$
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$ I $
represents the moment of inertia matrix.&lt;/li>
&lt;li>$ \boldsymbol{\omega} $
is the angular velocity vector (p, q, r).&lt;/li>
&lt;li>$ \mathbf{\tau} $
is the vector of external torques acting on the drone.&lt;/li>
&lt;/ul>
&lt;h3 id="dynamic-system-matrix">Dynamic System Matrix&lt;/h3>
&lt;p>The dynamics of the drone can be represented in a state-space format, where the state vector $ \mathbf{x} $
might include the position, velocity, orientation, and angular velocity. The state-space model is typically written as:&lt;/p>
$$
\dot{\mathbf{x}} = A\mathbf{x} + B\mathbf{u}
$$
$$
\mathbf{y} = C\mathbf{x} + D\mathbf{u}
$$
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$ \mathbf{x} $
is the state vector.&lt;/li>
&lt;li>$ \mathbf{u} $
is the input vector (like motor thrusts).&lt;/li>
&lt;li>$ \mathbf{y} $
is the output vector (like measured position and orientation).&lt;/li>
&lt;li>$ A $
is the system matrix, representing the dynamics of the drone.&lt;/li>
&lt;li>$ B $
is the input matrix, showing how inputs affect the state.&lt;/li>
&lt;li>$ C $
is the output matrix, linking the state to the outputs.&lt;/li>
&lt;li>$ D $
is the direct transmission matrix, usually a zero matrix in drone dynamics.&lt;/li>
&lt;/ul>
&lt;p>The matrices $$ A, B, C, D $$
are determined based on the physical characteristics of the drone and its operational environment. They encapsulate the dynamics and kinematics, converting control inputs into predictions about the drone&amp;rsquo;s future state.&lt;/p>
&lt;h2 id="2-control-system">2. Control System&lt;/h2>
&lt;h3 id="control-inputs">Control Inputs&lt;/h3>
&lt;p>The control inputs for the drone include thrust $$ U1 $$
and torques $$ U2, U3, U4 $$
. These inputs are calculated as follows:&lt;/p>
$$
U1 = c_t(\omega_1^2 + \omega_2^2 + \omega_3^2 + \omega_4^2)
$$
$$
U2 = c_t l (\omega_2^2 - \omega_4^2)
$$
$$
U3 = c_t l (\omega_3^2 - \omega_1^2)
$$
$$
U4 = c_q (-\omega_1^2 + \omega_2^2 - \omega_3^2 + \omega_4^2)
$$
&lt;h2 id="3-trajectory-generation">3. Trajectory Generation&lt;/h2>
&lt;p>The desired trajectory is represented as a function of time $ t $
:&lt;/p>
$$
X_{ref}(t), Y_{ref}(t), Z_{ref}(t), \psi_{ref}(t)
$$
&lt;h2 id="4-linear-parameter-varying-lpv-systems">4. Linear Parameter Varying (LPV) Systems&lt;/h2>
&lt;p>LPV systems are described by the following equations:&lt;/p>
$$
\dot{x}(t) = A(t)x(t) + B(t)u(t)
$$
$$
y(t) = C(t)x(t) + D(t)u(t)
$$
&lt;h2 id="5-model-predictive-control-mpc">5. Model Predictive Control (MPC)&lt;/h2>
&lt;p>Model Predictive Control (MPC) is an advanced method of process control that uses a dynamic model to predict and optimize future states of a control system. In the context of drone trajectory tracking, MPC is used to compute the optimal control inputs that will guide the drone along a desired trajectory. The basic formulation of an MPC problem can be given as follows:&lt;/p>
$$
\min_{u} \sum_{k=0}^{N-1} \left( (x_k - x_{ref})^T Q (x_k - x_{ref}) + (u_k - u_{ref})^T R (u_k - u_{ref}) \right)
$$
&lt;p>Subject to the dynamic constraints of the system:&lt;/p>
$$
x_{k+1} = A x_k + B u_k
$$
$$
x_{min} \leq x_k \leq x_{max}
$$
$$
u_{min} \leq u_k \leq u_{max}
$$
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$ x_k $
is the state of the system at step $ k $
.&lt;/li>
&lt;li>$ u_k $ is the control input at step $ k $.&lt;/li>
&lt;li>$ x_{ref} $ and $ u_{ref} $ are the reference state and input, respectively.&lt;/li>
&lt;li>$ Q $ and $ R $ are weighting matrices.&lt;/li>
&lt;li>$ N $ is the prediction horizon.&lt;/li>
&lt;li>$ A $ and $ B $ are the system matrices.&lt;/li>
&lt;li>$ x_{min}, x_{max}, u_{min}, u_{max} $ are the bounds on states and inputs.&lt;/li>
&lt;/ul>
&lt;h2 id="results">Results&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/S4rpkbglb5c?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div></description></item><item><title>🏠 Smart-Home-Using-IOT</title><link>http://localhost:1313/post/smart-home/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/smart-home/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>Project Name: Home Automation (Smart Home Using Internet of Things)&lt;/p>
&lt;p>Participatory Department: Electronics Engineering, Electrical Engineering&lt;/p>
&lt;p>Project Representative: Sai Navaneet&lt;/p>
&lt;p>Participants:&lt;/p>
&lt;ul>
&lt;li>Corporation: 1&lt;/li>
&lt;li>Professor: 1&lt;/li>
&lt;li>Undergraduate students: 2 (Manisha Lingala, Navaneet)&lt;/li>
&lt;/ul>
&lt;p>Period: March 1, 2022, to July 31, 2022 (5 months)&lt;/p>
&lt;p>Project Type: Middle&lt;/p>
&lt;h2 id="project-background">Project Background&lt;/h2>
&lt;p>The concept of home automation has been around for a long time, with the vision of fully automated homes and robotic assistance in household chores. However, the technology required to realize these ideas was not readily available until recent times. The project aims to leverage IoT technology to build a working intelligent home application, starting from scratch. The idea of smart homes has been experimented with since the early 19th century, aiming to make lives more comfortable.&lt;/p>
&lt;h2 id="objectives-and-content">Objectives and Content&lt;/h2>
&lt;p>The main objectives of the project are as follows:&lt;/p>
&lt;ol>
&lt;li>Increase comfort and quality of life in the house&lt;/li>
&lt;li>Enhance security and energy efficiency through remote-controllable equipment&lt;/li>
&lt;/ol>
&lt;p>To achieve these objectives, the following components are required:&lt;/p>
&lt;ul>
&lt;li>Relays&lt;/li>
&lt;li>Wi-Fi module (ESP32)&lt;/li>
&lt;li>Other basic electronic components&lt;/li>
&lt;/ul>
&lt;h2 id="project-results">Project Results&lt;/h2>
&lt;p>The project successfully achieved the following outcomes:&lt;/p>
&lt;ol>
&lt;li>Built a model of a house with basic appliances such as lights and fans&lt;/li>
&lt;li>Enabled remote control of these appliances through the cloud from anywhere in the world&lt;/li>
&lt;li>Implemented four types of control methods:
&lt;ul>
&lt;li>Control through the cloud&lt;/li>
&lt;li>Control through a mobile app&lt;/li>
&lt;li>Voice control&lt;/li>
&lt;li>Time-based control&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="circuit-diagram">Circuit Diagram&lt;/h2>
&lt;p>Below is the circuit diagram of the smart home system:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Circuit Diagram" srcset="
/post/smart-home/images/circuit_hu_f8d96e6f28afdbf3.webp 400w,
/post/smart-home/images/circuit_hu_37e193d227ca82a.webp 760w,
/post/smart-home/images/circuit_hu_a26d5b011283941e.webp 1200w"
src="http://localhost:1313/post/smart-home/images/circuit_hu_f8d96e6f28afdbf3.webp"
width="736"
height="402"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The circuit diagram showcases the connectivity of various components involved in the project. It includes the relays, Wi-Fi module (ESP32), and other basic electronic components. This diagram provides a visual representation of how the different elements of the system are interconnected to enable the desired functionality.&lt;/p>
&lt;h2 id="control-methods">Control Methods&lt;/h2>
&lt;p>The project implemented three different control methods for operating the smart home system:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Control through the Cloud:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through the Cloud" srcset="
/post/smart-home/images/appESP32_hu_d9b4391eb9516176.webp 400w,
/post/smart-home/images/appESP32_hu_d5aee5eb5624cc6f.webp 760w,
/post/smart-home/images/appESP32_hu_14e49d573187230a.webp 1200w"
src="http://localhost:1313/post/smart-home/images/appESP32_hu_d9b4391eb9516176.webp"
width="330"
height="722"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>This method allows users to control the smart home appliances using a web-based interface or application. Users can access the control interface through any internet-connected device, such as a computer or smartphone. It provides convenience and flexibility for managing the devices remotely, even in the absence of a mobile phone.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Control through Voice:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through Voice" srcset="
/post/smart-home/images/blynk_hu_8be3221733e347ab.webp 400w,
/post/smart-home/images/blynk_hu_f4ee17cbcea6fa2e.webp 760w,
/post/smart-home/images/blynk_hu_4108f99a85ca3457.webp 1200w"
src="http://localhost:1313/post/smart-home/images/blynk_hu_8be3221733e347ab.webp"
width="422"
height="156"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Voice control was implemented using Google Assistant and the IFTTT (If This Then That) platform. Users can give voice commands to control the devices. By assigning specific voice commands for turning on or off the appliances, users can conveniently operate the smart home system using their voice.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Control through Timer:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through Timer" srcset="
/post/smart-home/images/app2_hu_50e117d3edde1fc7.webp 400w,
/post/smart-home/images/app2_hu_a7fc3303b8e42e40.webp 760w,
/post/smart-home/images/app2_hu_5ac9831d355de566.webp 1200w"
src="http://localhost:1313/post/smart-home/images/app2_hu_50e117d3edde1fc7.webp"
width="258"
height="324"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The timer control method enables users to schedule the activation of devices at specific times. Users can set a particular time for the devices to turn on automatically. This feature is useful for automating routines and ensuring that appliances are activated at desired times without manual intervention.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="final-model">Final Model&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Final Model" srcset="
/post/smart-home/images/model_hu_e0af789f4923f697.webp 400w,
/post/smart-home/images/model_hu_78a754870aae4f1c.webp 760w,
/post/smart-home/images/model_hu_f3d306a512cd5abb.webp 1200w"
src="http://localhost:1313/post/smart-home/images/model_hu_e0af789f4923f697.webp"
width="451"
height="449"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>🏠 Smart-Home-Using-IOT</title><link>http://localhost:1313/project/smart-home-using-iot/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/project/smart-home-using-iot/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>Project Name: Home Automation (Smart Home Using Internet of Things)&lt;/p>
&lt;p>Participatory Department: Electronics Engineering, Electrical Engineering&lt;/p>
&lt;p>Project Representative: Sai Navaneet&lt;/p>
&lt;p>Participants:&lt;/p>
&lt;ul>
&lt;li>Corporation: 1&lt;/li>
&lt;li>Professor: 1&lt;/li>
&lt;li>Undergraduate students: 2 (Manisha Lingala, Navaneet)&lt;/li>
&lt;/ul>
&lt;p>Period: March 1, 2022, to July 31, 2022 (5 months)&lt;/p>
&lt;p>Project Type: Middle&lt;/p>
&lt;h2 id="project-background">Project Background&lt;/h2>
&lt;p>The concept of home automation has been around for a long time, with the vision of fully automated homes and robotic assistance in household chores. However, the technology required to realize these ideas was not readily available until recent times. The project aims to leverage IoT technology to build a working intelligent home application, starting from scratch. The idea of smart homes has been experimented with since the early 19th century, aiming to make lives more comfortable.&lt;/p>
&lt;h2 id="objectives-and-content">Objectives and Content&lt;/h2>
&lt;p>The main objectives of the project are as follows:&lt;/p>
&lt;ol>
&lt;li>Increase comfort and quality of life in the house&lt;/li>
&lt;li>Enhance security and energy efficiency through remote-controllable equipment&lt;/li>
&lt;/ol>
&lt;p>To achieve these objectives, the following components are required:&lt;/p>
&lt;ul>
&lt;li>Relays&lt;/li>
&lt;li>Wi-Fi module (ESP32)&lt;/li>
&lt;li>Other basic electronic components&lt;/li>
&lt;/ul>
&lt;h2 id="project-results">Project Results&lt;/h2>
&lt;p>The project successfully achieved the following outcomes:&lt;/p>
&lt;ol>
&lt;li>Built a model of a house with basic appliances such as lights and fans&lt;/li>
&lt;li>Enabled remote control of these appliances through the cloud from anywhere in the world&lt;/li>
&lt;li>Implemented four types of control methods:
&lt;ul>
&lt;li>Control through the cloud&lt;/li>
&lt;li>Control through a mobile app&lt;/li>
&lt;li>Voice control&lt;/li>
&lt;li>Time-based control&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="circuit-diagram">Circuit Diagram&lt;/h2>
&lt;p>Below is the circuit diagram of the smart home system:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Circuit Diagram" srcset="
/project/smart-home-using-iot/images/circuit_hu_f8d96e6f28afdbf3.webp 400w,
/project/smart-home-using-iot/images/circuit_hu_37e193d227ca82a.webp 760w,
/project/smart-home-using-iot/images/circuit_hu_a26d5b011283941e.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/circuit_hu_f8d96e6f28afdbf3.webp"
width="736"
height="402"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The circuit diagram showcases the connectivity of various components involved in the project. It includes the relays, Wi-Fi module (ESP32), and other basic electronic components. This diagram provides a visual representation of how the different elements of the system are interconnected to enable the desired functionality.&lt;/p>
&lt;h2 id="control-methods">Control Methods&lt;/h2>
&lt;p>The project implemented three different control methods for operating the smart home system:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Control through the Cloud:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through the Cloud" srcset="
/project/smart-home-using-iot/images/appESP32_hu_d9b4391eb9516176.webp 400w,
/project/smart-home-using-iot/images/appESP32_hu_d5aee5eb5624cc6f.webp 760w,
/project/smart-home-using-iot/images/appESP32_hu_14e49d573187230a.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/appESP32_hu_d9b4391eb9516176.webp"
width="330"
height="722"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>This method allows users to control the smart home appliances using a web-based interface or application. Users can access the control interface through any internet-connected device, such as a computer or smartphone. It provides convenience and flexibility for managing the devices remotely, even in the absence of a mobile phone.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Control through Voice:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through Voice" srcset="
/project/smart-home-using-iot/images/blynk_hu_8be3221733e347ab.webp 400w,
/project/smart-home-using-iot/images/blynk_hu_f4ee17cbcea6fa2e.webp 760w,
/project/smart-home-using-iot/images/blynk_hu_4108f99a85ca3457.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/blynk_hu_8be3221733e347ab.webp"
width="422"
height="156"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Voice control was implemented using Google Assistant and the IFTTT (If This Then That) platform. Users can give voice commands to control the devices. By assigning specific voice commands for turning on or off the appliances, users can conveniently operate the smart home system using their voice.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Control through Timer:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through Timer" srcset="
/project/smart-home-using-iot/images/app2_hu_50e117d3edde1fc7.webp 400w,
/project/smart-home-using-iot/images/app2_hu_a7fc3303b8e42e40.webp 760w,
/project/smart-home-using-iot/images/app2_hu_5ac9831d355de566.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/app2_hu_50e117d3edde1fc7.webp"
width="258"
height="324"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The timer control method enables users to schedule the activation of devices at specific times. Users can set a particular time for the devices to turn on automatically. This feature is useful for automating routines and ensuring that appliances are activated at desired times without manual intervention.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="final-model">Final Model&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Final Model" srcset="
/project/smart-home-using-iot/images/model_hu_e0af789f4923f697.webp 400w,
/project/smart-home-using-iot/images/model_hu_78a754870aae4f1c.webp 760w,
/project/smart-home-using-iot/images/model_hu_f3d306a512cd5abb.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/model_hu_e0af789f4923f697.webp"
width="451"
height="449"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item></channel></rss>