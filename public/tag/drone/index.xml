<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Drone | FLORENCIA GRATTAROLA</title><link>http://localhost:1313/tag/drone/</link><atom:link href="http://localhost:1313/tag/drone/index.xml" rel="self" type="application/rss+xml"/><description>Drone</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 06 Jun 2023 00:00:00 +0000</lastBuildDate><image><url>http://localhost:1313/media/icon_hu_2ac6985030d8f1fc.png</url><title>Drone</title><link>http://localhost:1313/tag/drone/</link></image><item><title>üéâ Drone Trajectory Tracking with Python</title><link>http://localhost:1313/post/drone-trajctory-tracking/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/drone-trajctory-tracking/</guid><description>&lt;h2 id="1-dynamics-and-kinematics-of-the-drone">1. Dynamics and Kinematics of the Drone&lt;/h2>
&lt;p>The dynamics and kinematics of a drone are crucial in understanding and controlling its flight behavior. The drone&amp;rsquo;s state can be described by its position $ x, y, z $
, orientation (roll $ \phi $
, pitch $ \theta $
, and yaw $ \psi $
), and their respective velocities and angular velocities.&lt;/p>
&lt;h3 id="translational-motion">Translational Motion&lt;/h3>
&lt;p>The translational motion of the drone can be described by Newton&amp;rsquo;s second law of motion:&lt;/p>
$$
m\ddot{\mathbf{r}} = \mathbf{F} - mg\hat{z}
$$
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$ m $
is the mass of the drone.&lt;/li>
&lt;li>$ \ddot{\mathbf{r}} $
represents the linear acceleration.&lt;/li>
&lt;li>$ \mathbf{F} $
is the total thrust force generated by the drone&amp;rsquo;s motors.&lt;/li>
&lt;li>$ g $
is the acceleration due to gravity.&lt;/li>
&lt;li>$ \hat{z} $
is the unit vector in the vertical direction.&lt;/li>
&lt;/ul>
&lt;h3 id="rotational-motion">Rotational Motion&lt;/h3>
&lt;p>The rotational motion of the drone is described using Euler&amp;rsquo;s equations of motion for a rigid body:&lt;/p>
$$
I \dot{\boldsymbol{\omega}} + \boldsymbol{\omega} \times (I \boldsymbol{\omega}) = \mathbf{\tau}
$$
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$ I $
represents the moment of inertia matrix.&lt;/li>
&lt;li>$ \boldsymbol{\omega} $
is the angular velocity vector (p, q, r).&lt;/li>
&lt;li>$ \mathbf{\tau} $
is the vector of external torques acting on the drone.&lt;/li>
&lt;/ul>
&lt;h3 id="dynamic-system-matrix">Dynamic System Matrix&lt;/h3>
&lt;p>The dynamics of the drone can be represented in a state-space format, where the state vector $ \mathbf{x} $
might include the position, velocity, orientation, and angular velocity. The state-space model is typically written as:&lt;/p>
$$
\dot{\mathbf{x}} = A\mathbf{x} + B\mathbf{u}
$$
$$
\mathbf{y} = C\mathbf{x} + D\mathbf{u}
$$
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$ \mathbf{x} $
is the state vector.&lt;/li>
&lt;li>$ \mathbf{u} $
is the input vector (like motor thrusts).&lt;/li>
&lt;li>$ \mathbf{y} $
is the output vector (like measured position and orientation).&lt;/li>
&lt;li>$ A $
is the system matrix, representing the dynamics of the drone.&lt;/li>
&lt;li>$ B $
is the input matrix, showing how inputs affect the state.&lt;/li>
&lt;li>$ C $
is the output matrix, linking the state to the outputs.&lt;/li>
&lt;li>$ D $
is the direct transmission matrix, usually a zero matrix in drone dynamics.&lt;/li>
&lt;/ul>
&lt;p>The matrices $$ A, B, C, D $$
are determined based on the physical characteristics of the drone and its operational environment. They encapsulate the dynamics and kinematics, converting control inputs into predictions about the drone&amp;rsquo;s future state.&lt;/p>
&lt;h2 id="2-control-system">2. Control System&lt;/h2>
&lt;h3 id="control-inputs">Control Inputs&lt;/h3>
&lt;p>The control inputs for the drone include thrust $$ U1 $$
and torques $$ U2, U3, U4 $$
. These inputs are calculated as follows:&lt;/p>
$$
U1 = c_t(\omega_1^2 + \omega_2^2 + \omega_3^2 + \omega_4^2)
$$
$$
U2 = c_t l (\omega_2^2 - \omega_4^2)
$$
$$
U3 = c_t l (\omega_3^2 - \omega_1^2)
$$
$$
U4 = c_q (-\omega_1^2 + \omega_2^2 - \omega_3^2 + \omega_4^2)
$$
&lt;h2 id="3-trajectory-generation">3. Trajectory Generation&lt;/h2>
&lt;p>The desired trajectory is represented as a function of time $ t $
:&lt;/p>
$$
X_{ref}(t), Y_{ref}(t), Z_{ref}(t), \psi_{ref}(t)
$$
&lt;h2 id="4-linear-parameter-varying-lpv-systems">4. Linear Parameter Varying (LPV) Systems&lt;/h2>
&lt;p>LPV systems are described by the following equations:&lt;/p>
$$
\dot{x}(t) = A(t)x(t) + B(t)u(t)
$$
$$
y(t) = C(t)x(t) + D(t)u(t)
$$
&lt;h2 id="5-model-predictive-control-mpc">5. Model Predictive Control (MPC)&lt;/h2>
&lt;p>Model Predictive Control (MPC) is an advanced method of process control that uses a dynamic model to predict and optimize future states of a control system. In the context of drone trajectory tracking, MPC is used to compute the optimal control inputs that will guide the drone along a desired trajectory. The basic formulation of an MPC problem can be given as follows:&lt;/p>
$$
\min_{u} \sum_{k=0}^{N-1} \left( (x_k - x_{ref})^T Q (x_k - x_{ref}) + (u_k - u_{ref})^T R (u_k - u_{ref}) \right)
$$
&lt;p>Subject to the dynamic constraints of the system:&lt;/p>
$$
x_{k+1} = A x_k + B u_k
$$
$$
x_{min} \leq x_k \leq x_{max}
$$
$$
u_{min} \leq u_k \leq u_{max}
$$
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$ x_k $
is the state of the system at step $ k $
.&lt;/li>
&lt;li>$ u_k $ is the control input at step $ k $.&lt;/li>
&lt;li>$ x_{ref} $ and $ u_{ref} $ are the reference state and input, respectively.&lt;/li>
&lt;li>$ Q $ and $ R $ are weighting matrices.&lt;/li>
&lt;li>$ N $ is the prediction horizon.&lt;/li>
&lt;li>$ A $ and $ B $ are the system matrices.&lt;/li>
&lt;li>$ x_{min}, x_{max}, u_{min}, u_{max} $ are the bounds on states and inputs.&lt;/li>
&lt;/ul>
&lt;h2 id="results">Results&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/S4rpkbglb5c?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div></description></item><item><title>üéâ Drone Trajectory Tracking with Python</title><link>http://localhost:1313/project/drone-trajectory-tracking/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/project/drone-trajectory-tracking/</guid><description>&lt;h2 id="1-dynamics-and-kinematics-of-the-drone">1. Dynamics and Kinematics of the Drone&lt;/h2>
&lt;p>The dynamics and kinematics of a drone are crucial in understanding and controlling its flight behavior. The drone&amp;rsquo;s state can be described by its position $ x, y, z $
, orientation (roll $ \phi $
, pitch $ \theta $
, and yaw $ \psi $
), and their respective velocities and angular velocities.&lt;/p>
&lt;h3 id="translational-motion">Translational Motion&lt;/h3>
&lt;p>The translational motion of the drone can be described by Newton&amp;rsquo;s second law of motion:&lt;/p>
$$
m\ddot{\mathbf{r}} = \mathbf{F} - mg\hat{z}
$$
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$ m $
is the mass of the drone.&lt;/li>
&lt;li>$ \ddot{\mathbf{r}} $
represents the linear acceleration.&lt;/li>
&lt;li>$ \mathbf{F} $
is the total thrust force generated by the drone&amp;rsquo;s motors.&lt;/li>
&lt;li>$ g $
is the acceleration due to gravity.&lt;/li>
&lt;li>$ \hat{z} $
is the unit vector in the vertical direction.&lt;/li>
&lt;/ul>
&lt;h3 id="rotational-motion">Rotational Motion&lt;/h3>
&lt;p>The rotational motion of the drone is described using Euler&amp;rsquo;s equations of motion for a rigid body:&lt;/p>
$$
I \dot{\boldsymbol{\omega}} + \boldsymbol{\omega} \times (I \boldsymbol{\omega}) = \mathbf{\tau}
$$
&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>$ I $
represents the moment of inertia matrix.&lt;/li>
&lt;li>$ \boldsymbol{\omega} $
is the angular velocity vector (p, q, r).&lt;/li>
&lt;li>$ \mathbf{\tau} $
is the vector of external torques acting on the drone.&lt;/li>
&lt;/ul>
&lt;h3 id="dynamic-system-matrix">Dynamic System Matrix&lt;/h3>
&lt;p>The dynamics of the drone can be represented in a state-space format, where the state vector $ \mathbf{x} $
might include the position, velocity, orientation, and angular velocity. The state-space model is typically written as:&lt;/p>
$$
\dot{\mathbf{x}} = A\mathbf{x} + B\mathbf{u}
$$
$$
\mathbf{y} = C\mathbf{x} + D\mathbf{u}
$$
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$ \mathbf{x} $
is the state vector.&lt;/li>
&lt;li>$ \mathbf{u} $
is the input vector (like motor thrusts).&lt;/li>
&lt;li>$ \mathbf{y} $
is the output vector (like measured position and orientation).&lt;/li>
&lt;li>$ A $
is the system matrix, representing the dynamics of the drone.&lt;/li>
&lt;li>$ B $
is the input matrix, showing how inputs affect the state.&lt;/li>
&lt;li>$ C $
is the output matrix, linking the state to the outputs.&lt;/li>
&lt;li>$ D $
is the direct transmission matrix, usually a zero matrix in drone dynamics.&lt;/li>
&lt;/ul>
&lt;p>The matrices $$ A, B, C, D $$
are determined based on the physical characteristics of the drone and its operational environment. They encapsulate the dynamics and kinematics, converting control inputs into predictions about the drone&amp;rsquo;s future state.&lt;/p>
&lt;h2 id="2-control-system">2. Control System&lt;/h2>
&lt;h3 id="control-inputs">Control Inputs&lt;/h3>
&lt;p>The control inputs for the drone include thrust $$ U1 $$
and torques $$ U2, U3, U4 $$
. These inputs are calculated as follows:&lt;/p>
$$
U1 = c_t(\omega_1^2 + \omega_2^2 + \omega_3^2 + \omega_4^2)
$$
$$
U2 = c_t l (\omega_2^2 - \omega_4^2)
$$
$$
U3 = c_t l (\omega_3^2 - \omega_1^2)
$$
$$
U4 = c_q (-\omega_1^2 + \omega_2^2 - \omega_3^2 + \omega_4^2)
$$
&lt;h2 id="3-trajectory-generation">3. Trajectory Generation&lt;/h2>
&lt;p>The desired trajectory is represented as a function of time $ t $
:&lt;/p>
$$
X_{ref}(t), Y_{ref}(t), Z_{ref}(t), \psi_{ref}(t)
$$
&lt;h2 id="4-linear-parameter-varying-lpv-systems">4. Linear Parameter Varying (LPV) Systems&lt;/h2>
&lt;p>LPV systems are described by the following equations:&lt;/p>
$$
\dot{x}(t) = A(t)x(t) + B(t)u(t)
$$
$$
y(t) = C(t)x(t) + D(t)u(t)
$$
&lt;h2 id="5-model-predictive-control-mpc">5. Model Predictive Control (MPC)&lt;/h2>
&lt;p>Model Predictive Control (MPC) is an advanced method of process control that uses a dynamic model to predict and optimize future states of a control system. In the context of drone trajectory tracking, MPC is used to compute the optimal control inputs that will guide the drone along a desired trajectory. The basic formulation of an MPC problem can be given as follows:&lt;/p>
$$
\min_{u} \sum_{k=0}^{N-1} \left( (x_k - x_{ref})^T Q (x_k - x_{ref}) + (u_k - u_{ref})^T R (u_k - u_{ref}) \right)
$$
&lt;p>Subject to the dynamic constraints of the system:&lt;/p>
$$
x_{k+1} = A x_k + B u_k
$$
$$
x_{min} \leq x_k \leq x_{max}
$$
$$
u_{min} \leq u_k \leq u_{max}
$$
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>$ x_k $
is the state of the system at step $ k $
.&lt;/li>
&lt;li>$ u_k $ is the control input at step $ k $.&lt;/li>
&lt;li>$ x_{ref} $ and $ u_{ref} $ are the reference state and input, respectively.&lt;/li>
&lt;li>$ Q $ and $ R $ are weighting matrices.&lt;/li>
&lt;li>$ N $ is the prediction horizon.&lt;/li>
&lt;li>$ A $ and $ B $ are the system matrices.&lt;/li>
&lt;li>$ x_{min}, x_{max}, u_{min}, u_{max} $ are the bounds on states and inputs.&lt;/li>
&lt;/ul>
&lt;h2 id="results">Results&lt;/h2>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/S4rpkbglb5c?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div></description></item><item><title>üè† Smart-Home-Using-IOT</title><link>http://localhost:1313/post/smart-home/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/post/smart-home/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>Project Name: Home Automation (Smart Home Using Internet of Things)&lt;/p>
&lt;p>Participatory Department: Electronics Engineering, Electrical Engineering&lt;/p>
&lt;p>Project Representative: Sai Navaneet&lt;/p>
&lt;p>Participants:&lt;/p>
&lt;ul>
&lt;li>Corporation: 1&lt;/li>
&lt;li>Professor: 1&lt;/li>
&lt;li>Undergraduate students: 2 (Manisha Lingala, Navaneet)&lt;/li>
&lt;/ul>
&lt;p>Period: March 1, 2022, to July 31, 2022 (5 months)&lt;/p>
&lt;p>Project Type: Middle&lt;/p>
&lt;h2 id="project-background">Project Background&lt;/h2>
&lt;p>The concept of home automation has been around for a long time, with the vision of fully automated homes and robotic assistance in household chores. However, the technology required to realize these ideas was not readily available until recent times. The project aims to leverage IoT technology to build a working intelligent home application, starting from scratch. The idea of smart homes has been experimented with since the early 19th century, aiming to make lives more comfortable.&lt;/p>
&lt;h2 id="objectives-and-content">Objectives and Content&lt;/h2>
&lt;p>The main objectives of the project are as follows:&lt;/p>
&lt;ol>
&lt;li>Increase comfort and quality of life in the house&lt;/li>
&lt;li>Enhance security and energy efficiency through remote-controllable equipment&lt;/li>
&lt;/ol>
&lt;p>To achieve these objectives, the following components are required:&lt;/p>
&lt;ul>
&lt;li>Relays&lt;/li>
&lt;li>Wi-Fi module (ESP32)&lt;/li>
&lt;li>Other basic electronic components&lt;/li>
&lt;/ul>
&lt;h2 id="project-results">Project Results&lt;/h2>
&lt;p>The project successfully achieved the following outcomes:&lt;/p>
&lt;ol>
&lt;li>Built a model of a house with basic appliances such as lights and fans&lt;/li>
&lt;li>Enabled remote control of these appliances through the cloud from anywhere in the world&lt;/li>
&lt;li>Implemented four types of control methods:
&lt;ul>
&lt;li>Control through the cloud&lt;/li>
&lt;li>Control through a mobile app&lt;/li>
&lt;li>Voice control&lt;/li>
&lt;li>Time-based control&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="circuit-diagram">Circuit Diagram&lt;/h2>
&lt;p>Below is the circuit diagram of the smart home system:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Circuit Diagram" srcset="
/post/smart-home/images/circuit_hu_f8d96e6f28afdbf3.webp 400w,
/post/smart-home/images/circuit_hu_37e193d227ca82a.webp 760w,
/post/smart-home/images/circuit_hu_a26d5b011283941e.webp 1200w"
src="http://localhost:1313/post/smart-home/images/circuit_hu_f8d96e6f28afdbf3.webp"
width="736"
height="402"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The circuit diagram showcases the connectivity of various components involved in the project. It includes the relays, Wi-Fi module (ESP32), and other basic electronic components. This diagram provides a visual representation of how the different elements of the system are interconnected to enable the desired functionality.&lt;/p>
&lt;h2 id="control-methods">Control Methods&lt;/h2>
&lt;p>The project implemented three different control methods for operating the smart home system:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Control through the Cloud:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through the Cloud" srcset="
/post/smart-home/images/appESP32_hu_d9b4391eb9516176.webp 400w,
/post/smart-home/images/appESP32_hu_d5aee5eb5624cc6f.webp 760w,
/post/smart-home/images/appESP32_hu_14e49d573187230a.webp 1200w"
src="http://localhost:1313/post/smart-home/images/appESP32_hu_d9b4391eb9516176.webp"
width="330"
height="722"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>This method allows users to control the smart home appliances using a web-based interface or application. Users can access the control interface through any internet-connected device, such as a computer or smartphone. It provides convenience and flexibility for managing the devices remotely, even in the absence of a mobile phone.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Control through Voice:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through Voice" srcset="
/post/smart-home/images/blynk_hu_8be3221733e347ab.webp 400w,
/post/smart-home/images/blynk_hu_f4ee17cbcea6fa2e.webp 760w,
/post/smart-home/images/blynk_hu_4108f99a85ca3457.webp 1200w"
src="http://localhost:1313/post/smart-home/images/blynk_hu_8be3221733e347ab.webp"
width="422"
height="156"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Voice control was implemented using Google Assistant and the IFTTT (If This Then That) platform. Users can give voice commands to control the devices. By assigning specific voice commands for turning on or off the appliances, users can conveniently operate the smart home system using their voice.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Control through Timer:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through Timer" srcset="
/post/smart-home/images/app2_hu_50e117d3edde1fc7.webp 400w,
/post/smart-home/images/app2_hu_a7fc3303b8e42e40.webp 760w,
/post/smart-home/images/app2_hu_5ac9831d355de566.webp 1200w"
src="http://localhost:1313/post/smart-home/images/app2_hu_50e117d3edde1fc7.webp"
width="258"
height="324"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The timer control method enables users to schedule the activation of devices at specific times. Users can set a particular time for the devices to turn on automatically. This feature is useful for automating routines and ensuring that appliances are activated at desired times without manual intervention.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="final-model">Final Model&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Final Model" srcset="
/post/smart-home/images/model_hu_e0af789f4923f697.webp 400w,
/post/smart-home/images/model_hu_78a754870aae4f1c.webp 760w,
/post/smart-home/images/model_hu_f3d306a512cd5abb.webp 1200w"
src="http://localhost:1313/post/smart-home/images/model_hu_e0af789f4923f697.webp"
width="451"
height="449"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>üè† Smart-Home-Using-IOT</title><link>http://localhost:1313/project/smart-home-using-iot/</link><pubDate>Tue, 06 Jun 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/project/smart-home-using-iot/</guid><description>&lt;h2 id="project-overview">Project Overview&lt;/h2>
&lt;p>Project Name: Home Automation (Smart Home Using Internet of Things)&lt;/p>
&lt;p>Participatory Department: Electronics Engineering, Electrical Engineering&lt;/p>
&lt;p>Project Representative: Sai Navaneet&lt;/p>
&lt;p>Participants:&lt;/p>
&lt;ul>
&lt;li>Corporation: 1&lt;/li>
&lt;li>Professor: 1&lt;/li>
&lt;li>Undergraduate students: 2 (Manisha Lingala, Navaneet)&lt;/li>
&lt;/ul>
&lt;p>Period: March 1, 2022, to July 31, 2022 (5 months)&lt;/p>
&lt;p>Project Type: Middle&lt;/p>
&lt;h2 id="project-background">Project Background&lt;/h2>
&lt;p>The concept of home automation has been around for a long time, with the vision of fully automated homes and robotic assistance in household chores. However, the technology required to realize these ideas was not readily available until recent times. The project aims to leverage IoT technology to build a working intelligent home application, starting from scratch. The idea of smart homes has been experimented with since the early 19th century, aiming to make lives more comfortable.&lt;/p>
&lt;h2 id="objectives-and-content">Objectives and Content&lt;/h2>
&lt;p>The main objectives of the project are as follows:&lt;/p>
&lt;ol>
&lt;li>Increase comfort and quality of life in the house&lt;/li>
&lt;li>Enhance security and energy efficiency through remote-controllable equipment&lt;/li>
&lt;/ol>
&lt;p>To achieve these objectives, the following components are required:&lt;/p>
&lt;ul>
&lt;li>Relays&lt;/li>
&lt;li>Wi-Fi module (ESP32)&lt;/li>
&lt;li>Other basic electronic components&lt;/li>
&lt;/ul>
&lt;h2 id="project-results">Project Results&lt;/h2>
&lt;p>The project successfully achieved the following outcomes:&lt;/p>
&lt;ol>
&lt;li>Built a model of a house with basic appliances such as lights and fans&lt;/li>
&lt;li>Enabled remote control of these appliances through the cloud from anywhere in the world&lt;/li>
&lt;li>Implemented four types of control methods:
&lt;ul>
&lt;li>Control through the cloud&lt;/li>
&lt;li>Control through a mobile app&lt;/li>
&lt;li>Voice control&lt;/li>
&lt;li>Time-based control&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="circuit-diagram">Circuit Diagram&lt;/h2>
&lt;p>Below is the circuit diagram of the smart home system:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Circuit Diagram" srcset="
/project/smart-home-using-iot/images/circuit_hu_f8d96e6f28afdbf3.webp 400w,
/project/smart-home-using-iot/images/circuit_hu_37e193d227ca82a.webp 760w,
/project/smart-home-using-iot/images/circuit_hu_a26d5b011283941e.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/circuit_hu_f8d96e6f28afdbf3.webp"
width="736"
height="402"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The circuit diagram showcases the connectivity of various components involved in the project. It includes the relays, Wi-Fi module (ESP32), and other basic electronic components. This diagram provides a visual representation of how the different elements of the system are interconnected to enable the desired functionality.&lt;/p>
&lt;h2 id="control-methods">Control Methods&lt;/h2>
&lt;p>The project implemented three different control methods for operating the smart home system:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Control through the Cloud:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through the Cloud" srcset="
/project/smart-home-using-iot/images/appESP32_hu_d9b4391eb9516176.webp 400w,
/project/smart-home-using-iot/images/appESP32_hu_d5aee5eb5624cc6f.webp 760w,
/project/smart-home-using-iot/images/appESP32_hu_14e49d573187230a.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/appESP32_hu_d9b4391eb9516176.webp"
width="330"
height="722"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>This method allows users to control the smart home appliances using a web-based interface or application. Users can access the control interface through any internet-connected device, such as a computer or smartphone. It provides convenience and flexibility for managing the devices remotely, even in the absence of a mobile phone.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Control through Voice:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through Voice" srcset="
/project/smart-home-using-iot/images/blynk_hu_8be3221733e347ab.webp 400w,
/project/smart-home-using-iot/images/blynk_hu_f4ee17cbcea6fa2e.webp 760w,
/project/smart-home-using-iot/images/blynk_hu_4108f99a85ca3457.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/blynk_hu_8be3221733e347ab.webp"
width="422"
height="156"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Voice control was implemented using Google Assistant and the IFTTT (If This Then That) platform. Users can give voice commands to control the devices. By assigning specific voice commands for turning on or off the appliances, users can conveniently operate the smart home system using their voice.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Control through Timer:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Control through Timer" srcset="
/project/smart-home-using-iot/images/app2_hu_50e117d3edde1fc7.webp 400w,
/project/smart-home-using-iot/images/app2_hu_a7fc3303b8e42e40.webp 760w,
/project/smart-home-using-iot/images/app2_hu_5ac9831d355de566.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/app2_hu_50e117d3edde1fc7.webp"
width="258"
height="324"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>The timer control method enables users to schedule the activation of devices at specific times. Users can set a particular time for the devices to turn on automatically. This feature is useful for automating routines and ensuring that appliances are activated at desired times without manual intervention.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="final-model">Final Model&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="Final Model" srcset="
/project/smart-home-using-iot/images/model_hu_e0af789f4923f697.webp 400w,
/project/smart-home-using-iot/images/model_hu_78a754870aae4f1c.webp 760w,
/project/smart-home-using-iot/images/model_hu_f3d306a512cd5abb.webp 1200w"
src="http://localhost:1313/project/smart-home-using-iot/images/model_hu_e0af789f4923f697.webp"
width="451"
height="449"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item><item><title>üéâ Drone Control with Keyboard</title><link>http://localhost:1313/project/drone-control-with-keyboard/</link><pubDate>Thu, 23 Mar 2023 00:00:00 +0000</pubDate><guid>http://localhost:1313/project/drone-control-with-keyboard/</guid><description>&lt;p>Developed a code to control a drone using a keyboard. It can
perform actions such as takeoff, landing, and controlling the
drone&amp;rsquo;s movements using the arrow keys and other designated keys.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/-XZIpyt2KPE?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Before running the code, make sure you have the following dependencies installed:&lt;/p>
&lt;ul>
&lt;li>Python 3.x&lt;/li>
&lt;li>Olympe&lt;/li>
&lt;li>pynput&lt;/li>
&lt;/ul>
&lt;p>You can install the required Python packages using pip:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">pip install olympe pynput
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="usage">Usage&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Connect to the drone&amp;rsquo;s Wi-Fi network.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Update the &lt;code>DRONE_IP&lt;/code> variable in the code to match the IP address of your drone. By default, it is set to &lt;code>10.202.0.1&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run the script by executing the following command:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">python script.py
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="4">
&lt;li>
&lt;p>The script will establish a connection with the drone and wait for your input.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Use the following keys to control the drone:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>t&lt;/strong>: Takeoff&lt;/li>
&lt;li>&lt;strong>l&lt;/strong>: Landing&lt;/li>
&lt;li>&lt;strong>a&lt;/strong>: Move left&lt;/li>
&lt;li>&lt;strong>d&lt;/strong>: Move right&lt;/li>
&lt;li>&lt;strong>w&lt;/strong>: Move forward&lt;/li>
&lt;li>&lt;strong>s&lt;/strong>: Move backward&lt;/li>
&lt;li>&lt;strong>up arrow&lt;/strong>: Increase altitude&lt;/li>
&lt;li>&lt;strong>down arrow&lt;/strong>: Decrease altitude&lt;/li>
&lt;li>&lt;strong>left arrow&lt;/strong>: Rotate left&lt;/li>
&lt;li>&lt;strong>right arrow&lt;/strong>: Rotate right&lt;/li>
&lt;li>&lt;strong>esc&lt;/strong>: Quit the script&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Press the corresponding keys to control the drone&amp;rsquo;s movements. The drone will respond accordingly.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To quit the script, press the &lt;strong>esc&lt;/strong> key.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Note:&lt;/strong> Make sure to fly the drone in a safe environment and comply with all local regulations and laws regarding drone usage.&lt;/p>
&lt;h2 id="license">License&lt;/h2>
&lt;p>This code is licensed under the &lt;a href="LICENSE">MIT License&lt;/a>. Feel free to modify and distribute it as needed.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;ul>
&lt;li>The Olympe library: &lt;a href="https://developer.parrot.com/docs/olympe/" target="_blank" rel="noopener">https://developer.parrot.com/docs/olympe/&lt;/a>&lt;/li>
&lt;li>The pynput library: &lt;a href="https://pynput.readthedocs.io/" target="_blank" rel="noopener">https://pynput.readthedocs.io/&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>TelloDrone with KeyBoard and Object detection</title><link>http://localhost:1313/project/tellodrone-with-keyboard/</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><guid>http://localhost:1313/project/tellodrone-with-keyboard/</guid><description>&lt;p>This project show how to control a Tello drone using your keyboard and perform object detection using a pre-trained model. The drone movements are controlled using the arrow keys and specific keys for takeoff, landing, and flips. The object detection is performed on the live video feed from the drone&amp;rsquo;s camera.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>Python 3.x&lt;/li>
&lt;li>&lt;code>getter&lt;/code> module&lt;/li>
&lt;li>&lt;code>djitellopy&lt;/code> library&lt;/li>
&lt;li>&lt;code>pygame&lt;/code> library&lt;/li>
&lt;li>&lt;code>cv2&lt;/code> (OpenCV) library&lt;/li>
&lt;li>&lt;code>cvzone&lt;/code> library&lt;/li>
&lt;/ul>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Clone the repository to your local machine:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">git clone https://github.com/sainavaneet/TelloDrone-with-KeyBoard-and-Objectdetection.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Install the required dependencies:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">pip install getter djitellopy pygame opencv-python cvzone
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="usage">Usage&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Import the necessary modules and initialize the required components:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">getter&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">kg&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">djitellopy&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">tello&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">time&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">sleep&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">kg&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">drone&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tello&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tello&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">drone&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">connect&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Define the function to get keyboard input for drone control:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">getKeyboardInput&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">isFlying&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># ... implementation ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">lr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fb&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ud&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">yw&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">isFlying&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Implement the drone control loop:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">while&lt;/span> &lt;span class="kc">True&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">getKeyboardInput&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">isFlying&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">isFlying&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">drone&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">send_rc_control&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sleep&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Implement object detection using the Tello drone&amp;rsquo;s camera:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ... import necessary modules ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">thres&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.50&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">nmsThres&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ... load class names and pre-trained model ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">while&lt;/span> &lt;span class="kc">True&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># ... get frame from drone&amp;#39;s camera ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># ... perform object detection ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># ... display the detected objects ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># ... get keyboard input and control the drone ...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># ... display the frame ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="videos">Videos&lt;/h2>
&lt;h3 id="object-detectioin">OBJECT DETECTIOIN&lt;/h3>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/FL7ku3DEp7o?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;h3 id="keyboard-control">KEYBOARD CONTROL&lt;/h3>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/IpdcT8TlTOE?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;h2 id="contributing">Contributing&lt;/h2>
&lt;p>Contributions are welcome! If you find any issues or have suggestions for improvements, please create a GitHub issue or submit a pull request.&lt;/p>
&lt;ul>
&lt;li>The Tello SDK provided by DJI Ryze Robotics: &lt;a href="https://github.com/damiafuentes/DJITelloPy" target="_blank" rel="noopener">DJITelloPy&lt;/a>&lt;/li>
&lt;li>Pre-trained object detection model: &lt;a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md" target="_blank" rel="noopener">TensorFlow Object Detection Model Zoo&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr></description></item></channel></rss>