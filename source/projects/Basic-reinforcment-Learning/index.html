
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Basic Reinforcement Learning Algorithms &#8212; Portfolio  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=d9d6623f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'source/projects/Basic-reinforcment-Learning/index';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.11.5/gsap.min.js"></script>
    <script src="../../../_static/custom.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Deep Neural Network Nonlinear Model Predictive Control for CSTR" href="../Deep%20Neural%20Network%20Nonlinear%20Model%20Predictive%20Control%20for%20CSTR/index.html" />
    <link rel="prev" title="TelloDrone with Keyboard and Object Detection" href="../TelloDrone%20with%20KeyBoard/index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Navaneet</p>
  
</a></div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/sainavaneet" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/in/sainavaneet76/" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../index.html">
                    Bio
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Resume/index.html">Resume</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../index.html">Projects</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../Tissue-processing/index.html">Transformer Based Vision Guided Tissue Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Autonomus%20Harvesting/index.html">üéâ Autonomus harvesting using Object Detecion</a></li>

<li class="toctree-l2"><a class="reference internal" href="../TelloDrone%20with%20KeyBoard/index.html">TelloDrone with Keyboard and Object Detection</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Basic Reinforcement Learning Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Deep%20Neural%20Network%20Nonlinear%20Model%20Predictive%20Control%20for%20CSTR/index.html">Deep Neural Network Nonlinear Model Predictive Control for CSTR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Drone%20Control%20with%20Keyboard/index.html">Drone Control with Keyboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Drone%20Trajectory%20Tracking/index.html">Drone Trajectory Tracking with Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Imitation-Learning-Franka/index.html">Combining Imitation Learning with Diffusion Processes on Robot Manipulator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LMPC-ILC/index.html">Hybrid Model Predictive and Iterative Learning Control for Enhanced Leader-Follower Robotic Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../32%20bit%20CPU%20based%20on%20MIPS%20architecture/index.html">32 bit CPU based on MIPS architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Smart%20Home%20Using%20Iot/index.html">Smart-Home-Using-IOT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Verilog/index.html">Verilog Projects</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../experience/index.html">üë®‚Äçüíª Experience</a></li>





<li class="toctree-l1"><a class="reference internal" href="../../publications/index.html">Publications</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Blog/Bluetooth/index.html">Blog</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Blog/Actfranka/index.html">Vision Guided Imitation Learning using Action Chunk Transformer</a></li>




<li class="toctree-l2"><a class="reference internal" href="../../Blog/isaacsim/index.html">Isaac Sim Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Blog/go2_robot_isaacsim/index.html">üêæ Go2 Robot in Isaac Sim (Omniverse)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../Contact/index.html">Contact Me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/source/projects/Basic-reinforcment-Learning/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Basic Reinforcement Learning Algorithms</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cartpole">Cartpole</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lunar-lander">Lunar Lander</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-reinforcement-learning">What is Reinforcement Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-in-action">Reinforcement Learning in action</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lunar-lander-environment">Lunar lander environment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dqn-algorithm">DQN Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training-and-hyperparameters-selection">Model Training and Hyperparameters Selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#result-analysis">Result analysis</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="basic-reinforcement-learning-algorithms">
<h1>Basic Reinforcement Learning Algorithms<a class="headerlink" href="#basic-reinforcement-learning-algorithms" title="Link to this heading">#</a></h1>
<p>Implemented some basic Rl examples</p>
<section id="cartpole">
<h2>Cartpole<a class="headerlink" href="#cartpole" title="Link to this heading">#</a></h2>
<iframe
  width="746"
  height="420"
  src="https://www.youtube.com/embed/Ka9Fwk6suv8?autoplay=1"
  title="Go2 Robot in Isaac Sim"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allow="autoplay; encrypted-media; fullscreen"
  allowfullscreen
></iframe>
</section>
<section id="lunar-lander">
<h2>Lunar Lander<a class="headerlink" href="#lunar-lander" title="Link to this heading">#</a></h2>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In this article, we will cover a brief introduction to <strong>Reinforcement Learning</strong> and will learn about how to train a <strong>Deep Q-Network(DQN)</strong> agent to solve the ‚Äú<em><strong>Lunar Lander</strong></em>‚Äù Environment in OpenAI gym.
We will use Google‚Äôs Deepmind and Reinforcement Learning Implementation for this.</p>
<p><img alt="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif" src="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif" /></p>
<p><strong>Reinforcement Learning</strong> is a massive topic and we are not going to cover everything here in detail. Instead, the aim of this article is to get your hands dirty with some practical example of reinforcement learning and show the implementation of RL in solving real-world use cases.</p>
<p>We will discuss the rationale behind using the DQN and will cover the Experience Replay and Exploration-Exploitation dilemma encountered while training the Neural Network is discussed as well. In the last, we will discuss the agent‚Äôs training and testing performance and the effect of hyper-parameter in the agent‚Äôs performance.</p>
<p>The full code can be found here on this <a class="reference external" href="https://github.com/fakemonk1/Reinforcement-Learning-Lunar_Lander">github</a> link.</p>
</section>
<section id="what-is-reinforcement-learning">
<h2>What is Reinforcement Learning?<a class="headerlink" href="#what-is-reinforcement-learning" title="Link to this heading">#</a></h2>
<p>Reinforcement learning is one of the most discussed, followed and contemplated topics in artificial intelligence (AI) as it has the potential to transform most businesses.</p>
<p>At the core of reinforcement learning is the concept that optimal behaviour or action is reinforced by a positive reward. Similar to toddlers learning how to walk who adjust actions based on the outcomes they experience such as taking a smaller step if the previous broad step made them fall, machines and software agents use reinforcement learning algorithms to determine the ideal behaviour based upon feedback from the environment. It‚Äôs a form of <a class="reference external" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> and therefore a branch of <a class="reference external" href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial intelligence</a>.</p>
</section>
<section id="reinforcement-learning-in-action">
<h2>Reinforcement Learning in action<a class="headerlink" href="#reinforcement-learning-in-action" title="Link to this heading">#</a></h2>
<p>An example of the reinforcement Learning in Action is <a class="reference external" href="https://deepmind.com/blog/alphago-zero-learning-scratch/">AlphaGo Zero</a> which was in the headlines in 2017. <a class="reference external" href="https://deepmind.com/blog/alphago-zero-learning-scratch/">AlphaGo</a> is a bot developed by Google that leveraged reinforcement learning and defeated a world champion at the ancient Chinese game of Go. This is the first time artificial intelligence (AI) defeated a professional Go player.  Go is considered much more difficult for computers to win than other games such as <a class="reference external" href="https://en.wikipedia.org/wiki/Chess">chess</a>, because its much larger <a class="reference external" href="https://en.wikipedia.org/wiki/Branching_factor">branching factor</a> makes it prohibitively difficult to use traditional AI methods such as <a class="reference external" href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning">alpha‚Äìbeta pruning</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Tree_traversal">tree traversal</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Heuristic">heuristic</a> search</p>
</section>
<section id="lunar-lander-environment">
<h2>Lunar lander environment<a class="headerlink" href="#lunar-lander-environment" title="Link to this heading">#</a></h2>
<p>We are using ‚ÄòLunar Lander‚Äô environment from OpenAI gym. This environment deals with the problem of landing a lander on a landing pad. The steps to set up this environment are mentioned in the OpenAI gym‚Äôs GitHub page [1] and on documentation link [2]. Following are the environment variables in brief to understand the environment we are working in.</p>
<ol class="arabic simple">
<li><p><strong>State</strong>: The state/observation is just the current state of the environment. There is an 8-dimensional continuous state space and a discrete action space.</p></li>
<li><p><strong>Action</strong>: For each state of the environment, the agent takes an action based on its current state. The agent can choose to take action from four discrete actions: do_nothing, fire_left_engine, fire_right_engine and fire_main_engine.</p></li>
<li><p><strong>Reward</strong>: The agent receives a small negative reward every time it carries out an action. This is done in an attempt to teach the agent to land the rocket as quickly and efficiently as possible. If the lander crashes or comes to rest, the episode is considered complete and it will be receiving additional -100 or +100 points depending on the outcome.</p></li>
</ol>
</section>
<section id="dqn-algorithm">
<h2>DQN Algorithm<a class="headerlink" href="#dqn-algorithm" title="Link to this heading">#</a></h2>
<p>The deep Q-learning algorithm that includes experience replay and œµ-greedy exploration is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>initialize replay memory R
initialize action-value function Q (with random weights)
observe initial state s
repeat
	select an action a
		with probability œµ select a random action
		otherwise select a= argmaxa‚Ä≤Q(s,a‚Ä≤)
	carry out action a
	observe reward rr and new state s‚Äô
	store experience &lt;s,a,r,s&gt; in replay memory R
	sample random transitions &lt;ss,aa,rr,ss‚Ä≤&gt;from replay memory R
	calculate target for each minibatch transition
		if ss‚Äô is terminal state then tt =rr otherwise tt =rr + Œ≥maxa‚Ä≤Q(ss‚Ä≤,aa‚Ä≤)
	train the Q network using (tt‚àíQ(ss,aa))2 as loss
	s=s‚Ä≤
until terminated
</pre></div>
</div>
</section>
<section id="model-training-and-hyperparameters-selection">
<h2>Model Training and Hyperparameters Selection<a class="headerlink" href="#model-training-and-hyperparameters-selection" title="Link to this heading">#</a></h2>
<p>For running the complete experiment for the ‚ÄòLunar Landing‚Äô environment, we will first train a benchmark model and then do more experiments to find out the effects of changing the hyperparameters on the model performance.</p>
<p>There is no rule of thumb to find out how many hidden layers you need in a neural network. I have conducted different experiments to try different combinations of node sizes for input and hidden layers. Following benchmark model was finalized on the basis of parameters like training time, number of episodes required for training and trained model performance.</p>
<ul class="simple">
<li><p><strong>Input layer</strong>: 512 nodes, observation_space count as input_dim and ‚Äòrelu‚Äô activation function</p></li>
<li><p><strong>Hidden layer</strong>: 1 layer, 256 nodes with ‚Äòrelu‚Äô activation function</p></li>
<li><p><strong>Output layer</strong>: 4 nodes, with ‚Äòlinear‚Äô activation function</p></li>
</ul>
<p><img alt="trained model" src="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/trained_model.png" /></p>
<p>Still, this model was sometimes diverging after an average reward of 170 and was taking more than 1000 episodes to diverge. I figured out that this behaviour might be attributed to overtraining of the model and implemented ‚Äò<strong>Early Stopping</strong>‚Äô. Early Stopping is the practice to stop the neural networks from overtraining. To implement this, I avoided training the model for a specific episode if the average of the last 10 rewards is more than 180.</p>
<p>Buffer capacity size is chosen of size 500000 to avoid overflow occurring because of large experience tuple. Model is trained for the maximum episode count of 2000 and stopping criteria for the trained model is the average reward of 200 for the last 100 episodes.</p>
<p>Final benchmark model has the following hyperparameters:</p>
<ul class="simple">
<li><p><em><strong>learning_rate</strong></em> =  0.001</p></li>
<li><p><em><strong>gamma</strong></em> = 0.99</p></li>
<li><p><em><strong>epsilon replay_memory_buffer_size</strong></em> = 500000</p></li>
<li><p><em><strong>epsilon_decay</strong></em> =  0.995</p></li>
</ul>
<p>Initially, as we can see below the agent is very bad at landing, it‚Äôs basically taking random actions and receives the negative rewards for crashing the rocket.</p>
<p><img alt="untrained_model" src="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/1.gif" /></p>
<p>After around 300 training episodes, it starts learning how to control and land the rocket.</p>
<p><img alt="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/2.gif" src="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/2.gif" /></p>
<p>After 600 the agent is fully trained. It learns to handle the rocket perfectly and lands the rocket perfectly each time.</p>
<p><img alt="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif" src="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif" /></p>
</section>
<section id="result-analysis">
<h2>Result analysis<a class="headerlink" href="#result-analysis" title="Link to this heading">#</a></h2>
<p>Figure 1. The reward for each training episode
<img alt="Figure 1" src="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/Figure_1_Reward%20for%20each%20training%20episode.png" /></p>
<p>Figure 1 shows the reward values per experience at the time of training. Blue lines denote the reward for each training episodes and the orange line shows the rolling mean of the last 100 episodes. The agent keeps learning with the time and the value of the rolling mean increases with the training episodes.</p>
<p>The average reward in the earlier episodes is mostly negative because the agent has just started learning. Eventually, The agent starts performing relatively better and the average reward starts going up and becoming positive after 300 episodes. After 514 episodes the rolling mean crosses 200 and the training concludes. There are a couple of episodes where the agent has received negative awards at this time, but I believe if the agent is allowed to continue training, these instances will reduce.</p>
<p>Figure 2. The reward for each testing episode
<img alt="" src="https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/Figure_2_Reward%20for%20each%20testing%20episode.png" /></p>
<p>Figure 2 shows the performance of the trained model for 100 episodes in the Lunar Lander environment. The trained model is performing well in the environment with all the rewards being positive. The average reward for 100 testing episodes is 205.</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../TelloDrone%20with%20KeyBoard/index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">TelloDrone with Keyboard and Object Detection</p>
      </div>
    </a>
    <a class="right-next"
       href="../Deep%20Neural%20Network%20Nonlinear%20Model%20Predictive%20Control%20for%20CSTR/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Deep Neural Network Nonlinear Model Predictive Control for CSTR</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cartpole">Cartpole</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lunar-lander">Lunar Lander</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-reinforcement-learning">What is Reinforcement Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-in-action">Reinforcement Learning in action</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lunar-lander-environment">Lunar lander environment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dqn-algorithm">DQN Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-training-and-hyperparameters-selection">Model Training and Hyperparameters Selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#result-analysis">Result analysis</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Navaneet
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025, Navaneet.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>